<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sonnet&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-02-23T08:27:51.826Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Pic by John Lennon</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>网站迁移(hexo)</title>
    <link href="http://yoursite.com/2018/02/20/%E7%BD%91%E7%AB%99%E8%BF%81%E7%A7%BB-hexo/"/>
    <id>http://yoursite.com/2018/02/20/网站迁移-hexo/</id>
    <published>2018-02-19T16:00:00.000Z</published>
    <updated>2018-02-23T08:27:51.826Z</updated>
    
    <content type="html"><![CDATA[<p>由于wordpress访问速度太慢，懒得折腾XD，把一些博客文章迁移到新博客上。目前使用Github Page + Hexo的方案比较流行，留作日常记录。</p><div align="center">The end</div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;由于wordpress访问速度太慢，懒得折腾XD，把一些博客文章迁移到新博客上。目前使用Github Page + Hexo的方案比较流行，留作日常记录。&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;The end&lt;/div&gt;

      
    
    </summary>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>2017年(Lunar Year)读书与看剧</title>
    <link href="http://yoursite.com/2018/02/15/Lunar_Year/"/>
    <id>http://yoursite.com/2018/02/15/Lunar_Year/</id>
    <published>2018-02-14T16:00:00.000Z</published>
    <updated>2018-02-23T11:38:48.132Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h3>读书（文学类）</h3><ul><li>加缪. 《鼠疫》</li><li>朱光潜.《西方美学史》 <em>读的慢，忘的快</em></li><li>乔治·奥威尔. 《1984》</li><li>乔治·奥威尔. 《动物农庄》</li><li>艾玛·拉金《在缅甸寻找乔治·奥威尔》</li><li>凯鲁亚克. 《在路上》</li><li>白先勇 《台北人》</li><li>尤瓦尔·赫拉利 《人类简史:从动物到上帝》</li><li>尤瓦尔, 赫拉利. 《未来简史》</li><li>斯蒂芬·茨威格. 《人类群星闪耀时》</li><li>尼古拉斯·斯帕克思. 《分手信》</li><li>卡勒德·胡赛尼. 《群山回唱》</li><li>费孝通. 《乡土中国》</li></ul><h3>读书（自我提高及工具书）</h3><ul><li>李忠秋. 《结构思考力》</li><li>尼尔·布朗《学会提问》</li><li>布鲁克·诺埃尔·摩尔《批判性思维》</li><li>读自我提高类书籍本意在于提升自身逻辑思考能力，上面几本书能够很好的达到目的。</li><li>Johnson. 实用多元统计分析.</li><li>RobertV.Hogg. 数理统计学导论</li><li>张维迎. 博弈论与信息经济学</li><li>Nasrabadi, Nasser M. “Pattern recognition and machine learning.”</li></ul><h3>看剧</h3><ul><li>《怦然心动》- 纯爱类影片，学会从整体审视生活</li><li>《时空恋旅人》 - 看完觉得幸福了一辈子</li><li>《饮食男女》 - 人之大欲，不过饮食男女</li><li>《闻香识女人》 - 善良、正直，是走得多远都不能丢弃的道德品质</li><li>《海边的曼彻斯特》 - 每个人都是一座孤独的岛</li><li>《东京爱情故事》 - 爱情是个很难说的事儿吧，thank you，赤名莉香</li><li>《白色巨塔》- 过程正义与结果正义</li><li>《NANA》 - 主题曲很好听</li><li>《熔炉》 - 不想评论，谢谢</li><li>《聚焦》 - 同上</li><li>《两杆大烟枪》 - 同《低俗小说》《疯狂的石头》，贵在叙事方式</li><li>《傲慢与偏见》、《罗马假日》、《南丁格尔》、《乱世佳人》还说什么，都是经典，无论看几遍，都能品出味道来.</li></ul><div align="center">和子由渑池怀旧 .苏轼<br>人生到处知何似，应似飞鸿踏雪泥。<br>泥上偶然留指爪，鸿飞那复计东西。<br>老僧已死成新塔，坏壁无由见旧题。<br>往日崎岖还知否，路长人困蹇驴嘶。<br></div>]]></content>
    
    <summary type="html">
    
      一个人不总是能让自己的工作有意义，或者同人类的伟大征程发生本质联系。创造出新东西是困难的，命运可能既没有提供资源和天分，也没有提供位置和环境。但反求诸己是可能的，去动手，去克服自己的积习，改造自己的反射，克服不断分泌的多巴胺的诱惑，意识到平庸并不等于无聊和自鸣得意，付出努力去获取知识，获取灵活的身体，理解自己在这个快速变动的世界上的角色，不是为了忍耐，而是为了找到自己的叙事
    
    </summary>
    
    
      <category term="文字" scheme="http://yoursite.com/tags/%E6%96%87%E5%AD%97/"/>
    
  </entry>
  
  <entry>
    <title>MacOS下使用python的多版本方案</title>
    <link href="http://yoursite.com/2018/01/01/MacOS%E4%B8%8B%E4%BD%BF%E7%94%A8python%E7%9A%84%E5%A4%9A%E7%89%88%E6%9C%AC%E6%96%B9%E6%A1%88/"/>
    <id>http://yoursite.com/2018/01/01/MacOS下使用python的多版本方案/</id>
    <published>2017-12-31T16:00:00.000Z</published>
    <updated>2018-02-23T17:03:29.984Z</updated>
    
    <content type="html"><![CDATA[<h2>背景</h2><p>MacOS系统本身自带python，但是版本仍然停留在python2.7。私以为python2与python3语言差别比较大，python3额外一些新特性如**“通配符**，字典可排序，统一的Unicode编码”**等，都值得去尝试。为此，保证电脑上两个版本都能共存是很必要的。</p><h2>方案一：使用pyenv兼容多版本</h2><p>pyenv 是轻量的Python版本管理器，帮助你在电脑上建立多个版本的python环境，并提供方便的切换方法。pyenv-virtualenv 是 pyenv的扩展工具（类Unix系统上），可以搭建虚拟且独立的python环境，可以使每个项目环境与其他项目独立开来，保持环境的干净，解决包冲突问题。</p><h3>1. 使用Mac OSX的 Homebrew 安装</h3><p>Homebrew作为OS X上强大的包管理器，为系统软件提供了非常方便的安装方式，独特式的解决了包的依赖问题，并不再需要烦人的sudo，一键式编译，无参数困扰，安装Homebrew：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~$ curl -L https://raw.githubusercontent.com/yyuu/pyenv-installer/master/bin/pyenv-installer | bash</span><br></pre></td></tr></table></figure><p>安装完成后，根据提示将如下语句加入到 <code>～/.bash_profile</code> 或<code>~/.bashrc</code> 中:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">port PYENV_ROOT=&quot;$HOME/.pyenv&quot;</span><br><span class="line">export PATH=&quot;$PYENV_ROOT/bin:$PATH&quot;</span><br><span class="line">eval &quot;$(pyenv init -)&quot;</span><br><span class="line">eval &quot;$(pyenv virtualenv-init -)&quot; # 这句可以不加</span><br></pre></td></tr></table></figure><h3>2. pyenv 常用命令</h3><p>使用 <code>pyenv commands</code> 显示所有可用命令</p><h4>python 安装与卸载</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~$ pyenv install 2.7.3   # 安装python</span><br><span class="line">~$ pyenv uninstall 2.7.3 # 卸载python</span><br></pre></td></tr></table></figure><h4>python切换</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~$ pyenv global 2.7.3  # 设置全局的 Python 版本，通过将版本号写入 ~/.pyenv/version 文件的方式。</span><br><span class="line">~$ pyenv local 2.7.3 # 设置 Python 本地版本，通过将版本号写入当前目录下的 .python-version 文件的方式。通过这种方式设置的 Python 版本优先级较 global 高。</span><br></pre></td></tr></table></figure><h4>python优先级</h4><p><strong>shell &gt; local &gt; global</strong></p><p>pyenv 会从当前目录开始向上逐级查找 .python-version 文件，直到根目录为止。若找不到，就用 global 版本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">~$ pyenv shell <span class="number">2.7</span><span class="number">.3</span> <span class="comment"># 设置面向 shell 的 Python 版本，通过设置当前 shell 的 PYENV_VERSION 环境变量的方式。</span></span><br><span class="line"><span class="comment"># 这个版本的优先级比 local 和 global 都要高。–unset 参数可以用于取消当前 shell 设定的版本。</span></span><br><span class="line">~$ pyenv shell --unset</span><br><span class="line">~$ pyenv rehash  <span class="comment"># 创建垫片路径（为所有已安装的可执行文件创建 shims，如：~/.pyenv/versions/*/bin/*，因此，每当你增删了 Python 版本或带有可执行文件的包（如 pip）以后，都应该执行一次本命令）</span></span><br></pre></td></tr></table></figure><h2>方案二：使用Anaconda包管理多版本python</h2><p>Anaconda 是 Python 的一个发行版，如果把 Python 比作 Linux，那么 Anancoda 就是 CentOS 或者 Ubuntu。它解决了Python开发者的两大痛点。</p><ul><li>提供包管理，功能类似于 pip，Windows 平台安装第三方包经常失败的场景得以解决。</li><li>提供虚拟环境管理，功能类似于 virtualenv，解决了多版本Python并存问题。###1. 下载 Anaconda直接在官网下载最新版本的 <a href="https://www.continuum.io/downloads" target="_blank" rel="noopener">https://www.continuum.io/downloads</a> 安装包， 选择对应Python版本的安装包，下载完成后直接安装，安装过程选择默认配置即可，大约需要1.8G的磁盘空间。</li></ul><p>conda 是 Anaconda 下用于包管理和环境管理的命令行工具，是 pip 和 vitualenv 的组合。安装成功后 conda 会默认加入到环境变量中，因此可直接在命令行窗口运行 <code>conda</code> 命令，命令帮助可通过<code>conda -h</code>查看。如果你熟悉 virtualenv，那么上手 conda 非常容易，不熟悉 virtulenv 的也没关系，它提供的命令就几个，非常简单。我们可以利用 conda 的虚拟环境管理功能在 Python2 和 Python3 之间自由切换。</p><h3>2. 多版本切换</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 基于 python2.7 创建一个名为py2_env 的环境</span><br><span class="line">conda create --name py2_env python=2.7</span><br><span class="line"></span><br><span class="line"># 基于 python3.6 创建一个名为py3_env 的环境</span><br><span class="line">conda create --name py3_env python=3.6 </span><br><span class="line"></span><br><span class="line"># 激活python环境</span><br><span class="line">activate py3_env  # windows</span><br><span class="line">source activate py3_env # linux/mac</span><br><span class="line"></span><br><span class="line"># 切换到python3</span><br><span class="line">activate py3_env</span><br></pre></td></tr></table></figure><h3>3. 包管理</h3><p>conda 的包管理功能是对 pip 的一种补充，如果当前已经激活了某个Python环境，那么就可以在当前环境开始安装第三方包。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 安装 numpy </span><br><span class="line">conda install numpy</span><br><span class="line"># 查看已安装的包</span><br><span class="line">conda list </span><br><span class="line"># 包更新</span><br><span class="line">conda update numpy</span><br><span class="line"># 删除包</span><br><span class="line">conda remove numpy</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      解决Python 2.x与Python 3.x版本共存的方案
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Mac" scheme="http://yoursite.com/tags/Mac/"/>
    
  </entry>
  
  <entry>
    <title>Python字符串处理拾掇</title>
    <link href="http://yoursite.com/2017/04/20/Python%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%A4%84%E7%90%86%E6%8B%BE%E6%8E%87/"/>
    <id>http://yoursite.com/2017/04/20/Python字符串处理拾掇/</id>
    <published>2017-04-19T16:00:00.000Z</published>
    <updated>2018-02-23T17:48:41.305Z</updated>
    
    <content type="html"><![CDATA[<p>Python 字符串操作（string替换、删除、截取、复制、连接、比较、查找、包含、大小写转换）</p><p>一、去空格及特殊符号</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s.strip().lstrip().rstrip(<span class="string">','</span>)</span><br></pre></td></tr></table></figure><p>二、复制字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># strcpy(sStr1,sStr2)</span></span><br><span class="line">sStr1 = <span class="string">'strcpy'</span></span><br><span class="line">sStr2 = sStr1</span><br><span class="line">sStr1 = <span class="string">'strcpy2'</span></span><br><span class="line">print(sStr2)</span><br></pre></td></tr></table></figure><p>三、连接字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#strcat(sStr1,sStr2)</span></span><br><span class="line">sStr1 = <span class="string">'strcat'</span></span><br><span class="line">sStr2 = <span class="string">'append'</span></span><br><span class="line">sStr1 += sStr2</span><br><span class="line">print(sStr1)</span><br></pre></td></tr></table></figure><p>四、查找字符</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#strchr(sStr1,sStr2)</span></span><br><span class="line"><span class="comment"># &lt; 0 为未找到</span></span><br><span class="line">sStr1 = <span class="string">'strchr'</span></span><br><span class="line">sStr2 = <span class="string">'s'</span></span><br><span class="line">nPos = sStr1.index(sStr2)</span><br><span class="line">print(nPos)</span><br></pre></td></tr></table></figure><p>五、比较字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#strcmp(sStr1,sStr2)</span></span><br><span class="line">sStr1 = <span class="string">'strchr'</span></span><br><span class="line">sStr2 = <span class="string">'strch'</span></span><br><span class="line">print(cmp(sStr1,sStr2))</span><br></pre></td></tr></table></figure><p>六、扫描字符串是否包含指定的字符</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#strspn(sStr1,sStr2)</span></span><br><span class="line">sStr1 = <span class="string">'12345678'</span></span><br><span class="line">sStr2 = <span class="string">'456'</span></span><br><span class="line"><span class="comment">#sStr1 and chars both in sStr1 and sStr2</span></span><br><span class="line">print(len(sStr1 <span class="keyword">and</span> sStr2))</span><br></pre></td></tr></table></figure><p>七、字符串长度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#strlen(sStr1)</span></span><br><span class="line">sStr1 = <span class="string">'strlen'</span></span><br><span class="line">print(len(sStr1))</span><br></pre></td></tr></table></figure><p>八、将字符串中的大小写转换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#strlwr(sStr1)</span></span><br><span class="line">sStr1 = <span class="string">'JCstrlwr'</span></span><br><span class="line">sStr1 = sStr1.upper()</span><br><span class="line"><span class="comment">#sStr1 = sStr1.lower()</span></span><br><span class="line">print(sStr1)</span><br></pre></td></tr></table></figure><p>九、追加指定长度的字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#strncat(sStr1,sStr2,n)</span></span><br><span class="line">sStr1 = <span class="string">'12345'</span></span><br><span class="line">sStr2 = <span class="string">'abcdef'</span></span><br><span class="line">n = <span class="number">3</span></span><br><span class="line">sStr1 += sStr2[<span class="number">0</span>:n]</span><br><span class="line">print(sStr1)</span><br></pre></td></tr></table></figure><p>十、字符串指定长度比较</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#strncmp(sStr1,sStr2,n)</span></span><br><span class="line">sStr1 = <span class="string">'12345'</span></span><br><span class="line">sStr2 = <span class="string">'123bc'</span></span><br><span class="line">n = <span class="number">3</span></span><br><span class="line">print( cmp(sStr1[<span class="number">0</span>:n],sStr2[<span class="number">0</span>:n]))</span><br></pre></td></tr></table></figure><p>十一、复制指定长度的字符</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#strncpy(sStr1,sStr2,n)</span></span><br><span class="line">sStr1 = <span class="string">''</span></span><br><span class="line">sStr2 = <span class="string">'12345'</span></span><br><span class="line">n = <span class="number">3</span></span><br><span class="line">sStr1 = sStr2[<span class="number">0</span>:n]</span><br><span class="line">print(sStr1)</span><br></pre></td></tr></table></figure><p>十二、将字符串前n个字符替换为指定的字符</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#strnset(sStr1,ch,n)</span></span><br><span class="line">sStr1 = <span class="string">'12345'</span></span><br><span class="line">ch = <span class="string">'r'</span></span><br><span class="line">n = <span class="number">3</span></span><br><span class="line">sStr1 = n * ch + sStr1[<span class="number">3</span>:]</span><br><span class="line">print(sStr1)</span><br></pre></td></tr></table></figure><p>十三、扫描字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#strpbrk(sStr1,sStr2)</span></span><br><span class="line">sStr1 = <span class="string">'cekjgdklab'</span></span><br><span class="line">sStr2 = <span class="string">'gka'</span></span><br><span class="line">nPos = <span class="number">-1</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> sStr1:</span><br><span class="line">    <span class="keyword">if</span> c <span class="keyword">in</span> sStr2:</span><br><span class="line">        nPos = sStr1.index(c)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">print( nPos)</span><br></pre></td></tr></table></figure><p>十四、翻转字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#strrev(sStr1)</span></span><br><span class="line">sStr1 = <span class="string">'abcdefg'</span></span><br><span class="line">sStr1 = sStr1[::<span class="number">-1</span>]</span><br><span class="line">print(sStr1)</span><br></pre></td></tr></table></figure><p>十五、查找字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#strstr(sStr1,sStr2)</span></span><br><span class="line">sStr1 = <span class="string">'abcdefg'</span></span><br><span class="line">sStr2 = <span class="string">'cde'</span></span><br><span class="line">print(sStr1.find(sStr2))</span><br></pre></td></tr></table></figure><p>十六、分割字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#strtok(sStr1,sStr2)</span></span><br><span class="line">sStr1 = <span class="string">'ab,cde,fgh,ijk'</span></span><br><span class="line">sStr2 = <span class="string">','</span></span><br><span class="line">sStr1 = sStr1[sStr1.find(sStr2) + <span class="number">1</span>:]</span><br><span class="line">print(sStr1)</span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">s = <span class="string">'ab,cde,fgh,ijk'</span></span><br><span class="line">print(s.split(<span class="string">','</span>))</span><br></pre></td></tr></table></figure><p>十七、连接字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">delimiter = <span class="string">','</span></span><br><span class="line">mylist = [<span class="string">'Brazil'</span>, <span class="string">'Russia'</span>, <span class="string">'India'</span>, <span class="string">'China'</span>]</span><br><span class="line">print(delimiter.join(mylist))</span><br><span class="line">PHP 中 addslashes 的实现</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addslashes</span><span class="params">(s)</span>:</span></span><br><span class="line">    d = &#123;<span class="string">'"'</span>:<span class="string">'\\"'</span>, <span class="string">"'"</span>:<span class="string">"\\'"</span>, <span class="string">"\0"</span>:<span class="string">"\\\0"</span>, <span class="string">"\\"</span>:<span class="string">"\\\\"</span>&#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(d.get(c, c) <span class="keyword">for</span> c <span class="keyword">in</span> s) </span><br><span class="line">s = <span class="string">"John 'Johny' Doe (a.k.a. \"Super Joe\")\\\0"</span></span><br><span class="line">print(s)</span><br><span class="line">print( addslashes(s))</span><br></pre></td></tr></table></figure><p>十八、只显示字母与数字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">OnlyCharNum</span><span class="params">(s,oth=<span class="string">''</span>)</span>:</span></span><br><span class="line">    s2 = s.lower();</span><br><span class="line">    fomart = <span class="string">'abcdefghijklmnopqrstuvwxyz0123456789'</span></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> s2:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> c <span class="keyword">in</span> fomart:</span><br><span class="line">            s = s.replace(c,<span class="string">''</span>);</span><br><span class="line">    <span class="keyword">return</span> s;</span><br><span class="line"> </span><br><span class="line">print(OnlyStr(<span class="string">"a000 aa-b"</span>))</span><br></pre></td></tr></table></figure><p>十九、截取字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">str = ’<span class="number">0123456789</span>′</span><br><span class="line"><span class="keyword">print</span> str[<span class="number">0</span>:<span class="number">3</span>] <span class="comment">#截取第一位到第三位的字符</span></span><br><span class="line"><span class="keyword">print</span> str[:] <span class="comment">#截取字符串的全部字符</span></span><br><span class="line"><span class="keyword">print</span> str[<span class="number">6</span>:] <span class="comment">#截取第七个字符到结尾</span></span><br><span class="line"><span class="keyword">print</span> str[:<span class="number">-3</span>] <span class="comment">#截取从头开始到倒数第三个字符之前</span></span><br><span class="line"><span class="keyword">print</span> str[<span class="number">2</span>] <span class="comment">#截取第三个字符</span></span><br><span class="line"><span class="keyword">print</span> str[<span class="number">-1</span>] <span class="comment">#截取倒数第一个字符</span></span><br><span class="line"><span class="keyword">print</span> str[::<span class="number">-1</span>] <span class="comment">#创造一个与原字符串顺序相反的字符串</span></span><br><span class="line"><span class="keyword">print</span> str[<span class="number">-3</span>:<span class="number">-1</span>] <span class="comment">#截取倒数第三位与倒数第一位之前的字符</span></span><br><span class="line"><span class="keyword">print</span> str[<span class="number">-3</span>:] <span class="comment">#截取倒数第三位到结尾</span></span><br><span class="line"><span class="keyword">print</span> str[:<span class="number">-5</span>:<span class="number">-3</span>] <span class="comment">#逆序截取，具体啥意思没搞明白？</span></span><br></pre></td></tr></table></figure><p>二十、直接贴过来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">str = <span class="string">"Hello My friend"</span></span><br><span class="line"><span class="comment"># 字符串是一个整体。如果你想直接修改字符串的某一部分，是不可能的。</span></span><br><span class="line"><span class="comment"># 但我们能够读出字符串的某一部分。</span></span><br><span class="line"><span class="comment"># 子字符串的提取</span></span><br><span class="line">str[:<span class="number">6</span>]</span><br><span class="line"><span class="comment"># 字符串包含判断操作符：in，not in</span></span><br><span class="line"><span class="string">"He"</span> <span class="keyword">in</span> str</span><br><span class="line"><span class="string">"she"</span> <span class="keyword">not</span> <span class="keyword">in</span> str</span><br><span class="line"></span><br><span class="line"><span class="comment"># string模块，还提供了很多方法，如</span></span><br><span class="line">S.find(substring, [start [,end]]) <span class="comment">#可指范围查找子串，返回索引值，否则返回-1</span></span><br><span class="line">S.rfind(substring,[start [,end]]) <span class="comment">#反向查找</span></span><br><span class="line">S.index(substring,[start [,end]]) <span class="comment">#同find，只是找不到产生ValueError异常</span></span><br><span class="line">S.rindex(substring,[start [,end]])<span class="comment">#同上反向查找</span></span><br><span class="line">S.count(substring,[start [,end]]) <span class="comment">#返回找到子串的个数</span></span><br><span class="line"></span><br><span class="line">S.lowercase()</span><br><span class="line">S.capitalize()      <span class="comment">#首字母大写</span></span><br><span class="line">S.lower()           <span class="comment">#转小写</span></span><br><span class="line">S.upper()           <span class="comment">#转大写</span></span><br><span class="line">S.swapcase()        <span class="comment">#大小写互换</span></span><br><span class="line"></span><br><span class="line">S.split(str, <span class="string">' '</span>)   <span class="comment">#将string转list，以空格切分</span></span><br><span class="line">S.join(list, <span class="string">' '</span>)   <span class="comment">#将list转string，以空格连接</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理字符串的内置函数</span></span><br><span class="line">len(str)                <span class="comment">#串长度</span></span><br><span class="line">cmp(<span class="string">"my friend"</span>, str)   <span class="comment">#字符串比较。第一个大，返回1</span></span><br><span class="line">max(<span class="string">'abcxyz'</span>)           <span class="comment">#寻找字符串中最大的字符</span></span><br><span class="line">min(<span class="string">'abcxyz'</span>)           <span class="comment">#寻找字符串中最小的字符</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># string的转换</span></span><br><span class="line">            </span><br><span class="line">float(str) <span class="comment">#变成浮点数，float("1e-1")  结果为0.1</span></span><br><span class="line">int(str)        <span class="comment">#变成整型，  int("12")  结果为12</span></span><br><span class="line">int(str,base)   <span class="comment">#变成base进制整型数，int("11",2) 结果为2</span></span><br><span class="line">long(str)       <span class="comment">#变成长整型，</span></span><br><span class="line">long(str,base)  <span class="comment">#变成base进制长整型，</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 字符串的格式化（注意其转义字符，大多如C语言的，略）</span></span><br><span class="line">str_format % (参数列表) <span class="comment">#参数列表是以tuple的形式定义的，即不可运行中改变</span></span><br><span class="line">&gt;&gt;&gt;<span class="keyword">print</span> <span class="string">""</span>%s<span class="string">'s height is %dcm" % ("My brother", 180)</span></span><br><span class="line"><span class="string">          #结果显示为 My brother'</span>s height <span class="keyword">is</span> <span class="number">180</span>cm</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      python中有些常用的字符串处理函数,特别是用python处理自然语言时，字符串处理用的十分频繁。下面总结一下常用的字符串处理的相关函数：
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>写在大三下学期</title>
    <link href="http://yoursite.com/2017/03/22/%E5%86%99%E5%9C%A8%E5%A4%A7%E4%B8%89%E4%B8%8B%E5%AD%A6%E6%9C%9F/"/>
    <id>http://yoursite.com/2017/03/22/写在大三下学期/</id>
    <published>2017-03-22T05:20:09.000Z</published>
    <updated>2018-02-23T11:47:54.891Z</updated>
    
    <content type="html"><![CDATA[<p>  大学时光经不起回味，一眨眼的时间，大学生生活已经步入尾声，随之而来的就该是毕业季的伤感了。步入大三的同学们，都在为步入”社会“摩拳擦掌，该考研的考研，该工作的工作。即使是平时班上最不爱学习的，也时不时开始向人打听起公务员考试的相关事宜了 :blush: :blush:</p><a id="more"></a><h2>时代的浪潮已经拍到了我们这代人的脚下</h2><p>  每每看到知乎上“大三了，考研还来得及吗？”，“大三了，学计算机还来得及吗？”诸如此类的问题，总也庆幸自己两年多的时间做了一些事读了一些书，但我知道自己做的还远远远远不够看。大三的生活，伴着毕业的步伐，来的匆匆。自己的学年论文加上向老师申请的论文，学的懵懂的专业课，还得挤出时间来为考研蓄力，冷不丁还夹带点感情的纠结。Flag ～</p><p>时光总是匆匆的催人老，情爱总是让人烦恼。</p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=394467&auto=0&height=66"></iframe><h2>写博客的初衷</h2><p>  大脑的存储空间是有限的，为了让新知识进来，你就得把暂时用不上旧知识遗忘掉。但是，自己当时花了那么多宝贵时间，一步一步摸索着过来所接触到的总结过的知识点，就这样被遗忘，或者被藏在某本笔记本的角落是在太可惜。当你怀着一颗热忱的心去进入一个领域，你只有从宏观上清楚了行业整体，从微观上把握住各个知识点原理，才算得上入门选手。</p><p>  为此，记录下来并且可查可复习非常重要，这也就是Blog能为我提供的。</p><p><em>性格决定你做怎样的选择，你的选择决定你怎样的人生。生命只有一次，而且它正在以不可逆转的姿态向前推进。你可以去旅行，可以朝九晚五地上班，还可以不顾一切地去创业。只是，千万不要选择平庸的人生。</em></p><blockquote><p>黄色的树林里分出两条路，可惜我不能同时去涉足，我在那路口久久伫立，我向着一条路极目望去，直到它消失在丛林深处。</p></blockquote><blockquote><p>但我选了另外一条路，它荒草萋萋，十分幽寂，显得更诱人，更美丽；虽然在这条小路上，很少留下旅人的足迹。</p></blockquote><blockquote><p>那天清晨落叶满地，两条路都未经脚印污染。啊，留下一条路等改日再见！但我知道路径延绵无尽头，恐怕我难以再回返。</p></blockquote><blockquote><p>也许多少年后在某个地方，我将轻声叹息将往事回顾：一片树林里分出两条路——而我选择了人迹更少的一条，从此决定了我一生的道路。——罗伯特·弗罗斯特《未选择的路》</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;  大学时光经不起回味，一眨眼的时间，大学生生活已经步入尾声，随之而来的就该是毕业季的伤感了。步入大三的同学们，都在为步入”社会“摩拳擦掌，该考研的考研，该工作的工作。即使是平时班上最不爱学习的，也时不时开始向人打听起公务员考试的相关事宜了 :blush: :blush:&lt;/p&gt;
    
    </summary>
    
    
      <category term="文字" scheme="http://yoursite.com/tags/%E6%96%87%E5%AD%97/"/>
    
  </entry>
  
  <entry>
    <title>「如何高效的学习」读书笔记</title>
    <link href="http://yoursite.com/2017/03/22/%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2017/03/22/如何高效学习/</id>
    <published>2017-03-21T16:00:00.000Z</published>
    <updated>2018-02-23T10:35:11.579Z</updated>
    
    <content type="html"><![CDATA[<p>《如何高效学习》从学习的战略上指导学习知识的策略抛出整体性学习的概念，以及结构、模型、高速公路的观点;详细分析了学习的顺序和信息的分类。以战术的方式分解学习的各种战术，如快速阅读、笔记流、比喻法、内在化、联想法、图表法、挂钩法、信息压缩、知识应用、模型纠错等。下面是我阅读时写下的感想，比较少的书摘。</p><a id="more"></a><h3>1. 类比</h3><p>首先，类比是一个推理方法。<br>其次，类比是把两个具有相似或者相同特征的对象进行推理它们具有其他相似或相同特征的方法。</p><h3>2. 概括</h3><p>   读书，将核心用词语总结概括，是最好的学习习惯，只有这样，知识才真正理解，被你加工，通过联想、概括出自己的观点和想法，知识才被刚刚固化，为了知识更加牢固，还需要将知识进行实践运用，知识最好的实践方式就是传播，传播有时候会收到很多不同意见。相互碰撞，如果你能简单轻松的让他们理解并认可你的观点，说明你基本吃透这个知识点。他们当中的不同声音会让你更加容易固化你的知识点，尝试让他们也信服你如果你是正确的。</p><h3>3. 知识纵向拓展</h3><p>   知识纵向拓展，寻根究底。如学习企业管理知识，企业管理包含了财务、人事等管理，那么财务管理是怎样的，人事管理又是怎样的?再深入财务和人事管理里面的各种细节。结合自己所在公司，思考为什么要这样管理，自己公司是这样管理的吗?别人公司是怎样管理的，自己公司和别的公司管理上有什么差别，如何改进?这就是深度拓展，不断深挖细节。</p><h3>4. 知识横向拓展</h3><p>   知识横向拓展，昨天在印象笔记整理文章的时候，正好看到营销这个词，于是就把各类营销词汇整理了一番，方便以后学习归档，如病毒营销、邮件营销、关系营销、内容营销、事件营销等等，可能里面多个营销方式有很多共同点，分别找出来，也有不同点把它们区别开，这些营销方式适合什么场景下使用，适合企业的什么阶段使用，需要哪些辅助工作才能完成，成本多少?这就是横向拓展，通过知识与知识的联系发现他们共同点和不同点，找出他们的差异化，其实横向拓展和纵向拓展是不会绝对分开，当你开始比较的时候，大脑其实就进入了深度拓展和横向拓展，多思考多关联多实践多PK。</p><h3>5. 结构：理解就是结构高度发达完善的结果。</h3><p>   理解的重要性，一个人看书是否学到东西，看他的笔记和读后感就知道，如果这个人笔记和读后感总结都是勾画的原作者的文字，这是较少理解，不够深入，最差的理解是读书笔记和读后感都没有的，也是最差的学习方式，刚刚说的这些话都是错误的!<br>   真正理解，是可以通过结构化思维表达出来。理解帮忙我们把知识结构化，总感觉这里翻译的有问题，只有理解后的知识才便于储存，便于在大脑里结构化。怎么来理解呢?举个例子<br>   当我们阅读&quot;理解就是结构高度发达完善的结果&quot;的时候，如果很快就弄懂了这句话，这句话可能就相当于电脑内存中的一行数据，当我们阅过这句话，看到下段文字的时候这句话已经从内存中消失。但恰巧我读到时候，有意识觉得这里可以写点什么，就反复读了几遍，理解是结构发达的结果，理解是名词，结构发达是短语，结果是名词是产出物，正确说法是因为〖怎么样〗所以〖造成这个结果〗，那么就应该是因为理解所以造成结构发达的结果。修改后：结构高度发达完善就是理解的结果。<br>   有点咬文嚼字，扯远了，当我开始纠结这句话，大脑其实在高速运转，调动了我大脑磁盘里的相关信息来帮助我理解这句话，并做出推断。然后这句话就会固化在我的大脑磁盘里，不是一闪而过的内存数据，同时我刚刚调用过的知识再次被激活得以运用，知识就是越来应用的，不用就不属于你的知识。<br>   我不敢说解释得很明白，希望你们能够看到我对一字一句的理解，以及我狭小硬盘储存的不是特别完善的知识结构是如何被调用的。<br>   我在这里有意的写笔记是为了培养以后习惯性的思考。</p><h3>6. 模型：模型就像是结构的种子，是一座建筑的地基和框架，是知识最核心的概念，在此基础上将引伸出全部的知识。</h3><p>   先回顾，整体性学习的三个重要概念：结构，模型，高速公路。<br>   当我读完模型这一小节，我的脑子里浮现出类似原子的东西，在黑色的空间里自由漂浮，每个原子并不孤立，它们之间有一条耀眼的类似光线的&quot;脐带&quot;互相交错连接，这条连接线经常是一团亮斑从原子这头快速移动到另一头，所以看整体非常耀眼，其实这就是知识的传递与连接。<br>   现在看来，模型就是知识的最小结构，知识的结构由无数的模型和连接组成，我不知道我理解的连接是不是笔者的高速路公路，带着问题继续往下读，等待犹如开奖的那一惊心动魄。</p><h3>7. 高速公路：结构与结构之间的联系。</h3><p>   高速公路的比喻用得非常精准，这是城际之间的快速通道，是连接结构与结构之间的线路。这条线路传递和连接的知识(不是知识点，模型才是知识点)将以光速进行传递，也就是我们常说的一念之间。<br>   结构是模型的集合，模型与模型之间相互交错连接，结构与结构之间交错连接。知识点互相关联，知识互相连接交错。完美的一幅图画，黑暗的空间，一个个大型的分子，分子是若干原子的集合，是原子的仓库，不断有新的原子增加或者删除，我能清晰的透过分子的透薄如水的墙壁看到原子瞬间增加和破灭的过程，每个原子其中的若干连接线突然断裂，它在瞬间被孤立最后立即消亡，这是一个正常情况无法看到感知到的过程。分子的存亡和原子的存活方式完全不一样，只要有一个原子存在，依靠这类原子组成的分子就不会死亡。</p><h3>8. 结构分类</h3><p>   成熟结构、感知结构、生活经验、关系结构、基础数学结构</p><h3>9. 学习顺序</h3><p>   获取、理解、拓展、纠错、应用，测试伴随以上每一步，总结自省伴随以上每一步。</p><h3>10. 信息获取</h3><p>   信息获取，信息大爆炸的今天我们随时都能接触到海量信息，信息获取渠道非常丰富，百度、百科、知道、知乎、各类APP、新闻网站、社交媒体、自媒体等等，所以我们要提高信息获取的手段和筛选方法。<br></p><ul><li>订阅，订阅自己关注的内容或则话题</li><li>筛选，使用印象笔记或有道云笔记以及剪藏功能，把自己粗读有价值的信息收集整理，记得设置标签，方便以后索引。</li><li>(3) 定时整理笔记工具里的内容，整理成文章总结，将信息进行归类。总结的时候可以根据内容进行发散联想，拓展更广的话题，写下来整理成文章，这就是你的学习所得。</li></ul><h3>11. 信息获取方法</h3><p>   精简信息、增加信息获取的数量和信息来源、提高阅读速度效率</p><h3>12. 信息拓展方法</h3><p>   深度拓展、纵向拓展、横向拓展</p><h3>13.信息获取小结</h3><p>   一句话概括，通过不同手段获取知识，不断理解深入研究，拓展和联想相关知识，不断纠错判断，沉淀正确的知识并实践应用。<br>   拓展知识有三个方法，深度拓展、横向拓展、纵向拓展，深度拓展又可以理解为背景拓展，主要了解知识形成的背景、原因和过程，深度拓展和纵向理解容易造成误会，所以固化背景拓展最佳。<br>   关于信息获取每一步的测试单独说下：</p><ul><li>获取——是否看过听过，如果看过比较了解可以略过，进入初期筛选</li><li>理解——真的明白了知识点的含义了吗?是否可以用最简单的语言和比喻解释清楚</li><li>拓展——知识背景如何，相关知识有哪些，存在什么关系</li><li>纠错——哪些知识点、观点是错误的，正确的是什么?为什么它是错的</li><li>运用——这些知识如何运用到现实生活?有什么意义和价值?</li></ul><h3>14. 信息分类</h3><p>   随意信息、观点信息、过程信息、具体信息、抽象信息</p><h3>15. 挂钩法</h3><p>   刚刚我做了一个关于挂钩法有趣的小实验，用了大概二十秒钟记住8件东西，缺了两样，但是我觉得挂钩法依然非常有效。<br>   这几样东西分别是：</p><table><thead><tr><th style="text-align:center">培根</th></tr></thead><tbody><tr><td style="text-align:center">鸡蛋</td></tr><tr><td style="text-align:center">葡萄酒</td></tr><tr><td style="text-align:center">电池</td></tr><tr><td style="text-align:center">泡泡糖</td></tr><tr><td style="text-align:center">牛奶</td></tr><tr><td style="text-align:center">信封</td></tr><tr><td style="text-align:center">菠菜</td></tr><tr><td style="text-align:center">咖啡</td></tr><tr><td style="text-align:center">番茄</td></tr></tbody></table><p>   然后在一边分别把他们列出来。我是这样记忆的，时不时我上班会去早餐店买一份叫培根煎饼的早餐，里面有培根、鸡蛋、生菜、沙拉酱或者番茄酱或则辣椒酱，我就把生菜巧妙替换成菠菜，煎饼比较干那么肯定得来杯牛奶，营养又健康，和牛奶一样的饮品(联想法)有我平时最爱喝的咖啡，最近我还在学习做牛扒，家里一直缺红酒，红酒也是装逼利器哟，吃完早餐喝完咖啡，心里还想着牛扒红酒高逼格的美食，也该进入工作状态(继续挂钩)，打开邮件(映射信封)，查看有没有工作方面的事情，一边嚼着同事给的口香糖，其实我从不买泡泡糖那种口香糖，太甜越嚼越没味道，回到家吃了这么多好吃的肯定担心自己胖了没就需要称称，恰好，电子称没电了，一直拖着没去买电池。   写了好长时间了啊，实际上就是脑子里一瞬间的联想和挂钩。</p><h3>16. 内在化</h3><p>   通过将信息转化为更容易想象的形式，你可以为知识建立广泛的联系</p><h3>17. 内化与内在化</h3><p>   本文用词稍微不太严谨，前面提到的是内在化，这里提的是内化。</p><ul><li>知识内化，是企业管理的重要组成部分，按字面意思理解就是知识的内部消化吸收再创新。</li><li>知识内在化，我又只有闭眼冥想我那黑暗的知识空间，存在无数的知识分子和原子，当我获得新的知识点(原子)的时候就会通过光速连接(本书中的高速公路)找到适合该原子归类的仓库(分子)，如果这不是简单的知识点本身可以独立成新的知识，会自动产生一个分子并包含最少一个原子，这个分子与刚刚通过的光速连接那端的分子自动连接建立索引，便于以后快速搜索及时反馈。继续扩展下内化的过程是不是和搜索引擎很像，检索新的网页(信息收集)然后通过算法判断是否索引或者更新快照(理解、纠错)，蜘蛛沿着网页入口进行纵向和横向爬行(拓展)然后重复算法的判断，合格的网页进行存储并排序展示给用户(应用)。</li></ul><h3>18. 学习的战略战术</h3><p>   简单说战略指导思想，战术指导方法。学习知识的战略指导你把知识融会贯通，运用到现实生活和工作中，学习知识的战术指导你如何去学习。</p><ul><li>获取知识：快速阅读、笔记流</li><li>联系观点：比喻法、内在化、图表法</li><li>随意信息处理：联想法、挂钩法、信息压缩</li><li>知识拓展：知识应用、模型纠错、以项目为基础学习</li></ul><h3>19. 笔记流</h3><p>   &quot;一次学会&quot;表示你在学习时要全神贯注地听老师讲课，而不是忙着做细致漂亮的笔记，等到课下再学。工作中，尤其是会议，用关键词记录，会议后整理，效率高。<br>   一旦你写下了一个观点，下一步就是在这个观点和其他观点之间画上一些箭头呈现出相互关联的关系，形成一张观点网络。</p><h3>20. 读书笔记</h3><p>   写读书笔记是最好的练习方式，把勾画的重点和自己的观点整理出来，在通过词汇概括，用一句话吧所有重点以词汇的方式串联起来，再分别详细介绍每个词汇背后的观点，以及引用的文摘，可用图表辅助概括总结比较零散的结构。</p><h3>21. 费曼技巧</h3><ul><li>选择要学习的内容</li><li>以老师的身份学习</li><li>疑惑时返回学习</li><li>用简单直白和比喻去解释知识点</li></ul><h3>22. 提高效率</h3><ul><li>健康身心</li><li>聪明学习</li><li>不拖延</li><li>批处理碎片信息</li><li>坚持做清单、写日子</li></ul><h3>23. 自我教育</h3><ul><li>养成阅读习惯</li><li>设定学习目标写读书笔记是最好的练习方式，把勾画的重点和自己的观点整理出来，在通过词汇概括，用一句话吧所有重点以词汇的方式串联起来，再分别详细介绍每个词汇背后的观点，以及引用的文摘，可用图表辅助概括总结比较零散的结构。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《如何高效学习》从学习的战略上指导学习知识的策略抛出整体性学习的概念，以及结构、模型、高速公路的观点;详细分析了学习的顺序和信息的分类。以战术的方式分解学习的各种战术，如快速阅读、笔记流、比喻法、内在化、联想法、图表法、挂钩法、信息压缩、知识应用、模型纠错等。下面是我阅读时写下的感想，比较少的书摘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="文字" scheme="http://yoursite.com/tags/%E6%96%87%E5%AD%97/"/>
    
      <category term="阅读" scheme="http://yoursite.com/tags/%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>SVM支持向量机</title>
    <link href="http://yoursite.com/2017/02/27/SVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>http://yoursite.com/2017/02/27/SVM支持向量机/</id>
    <published>2017-02-26T16:00:00.000Z</published>
    <updated>2018-02-23T11:50:23.851Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>通俗来讲，SVM是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。</p></blockquote><a id="more"></a><h2>从线性回归讲起</h2><p>SVM主要是用来做分类工作，诸如文本分类，图像分类，生物序列分析和生 物数据挖掘， 手写字符识别等领域都有很多的应用。</p><p>对分类最简单的即线性分类器用X表示数据点，Y表示类别（二分类中，y取1或-1），一个线性分类器的目标是在数据空间中找到一个分隔平面,这个分隔平面方程可以表示为：$$\omega^{T}x+b=0$$为使目标函数值在-1到1之间，我们使用Logistic函数作为假设函数。</p><p>假设函数：</p><p>$$h_\theta(x)=g(\theta^{T}x)=\frac{1}{1+e^(-\theta^{T}x)}, \quad h_\theta(x)\in(0,1)$$</p><p>其中，x是n维特征向量，所以假设函数就是y=1的概率：</p><p>$$P(y=1|x;\theta)=h_\theta(x) \ P(y=0|x;\theta)=1-h_\theta(x)$$</p><p>从而，有$h_\theta(x)&gt;0.5$就是y=1的类，反之属于y=0的类。接下来，将结果中y = 0 和 y = 1 替换为 y =-1，y = 1，然后将$\theta^Tx= \theta_0x_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n(x_0=1)$中的$x_0$替换 为 b，最后将后面的$\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n(x_0=1)$替换为$\omega^Tx$，也就是说除了 y 由 y = 0 变为 y =1 外，线性分类函数跟 Logistic 回归的形式化表示$h_\theta(x)=g(\theta^Tx)=g(\omega^Tx+b)$没区别。<img src="http://img.blog.csdn.net/20170226133437427?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><h3>函数间隔与几何间隔</h3><p>在超平面 $\omega^{T} x + b = 0$ 确定的情况下，$|\omega^{T} x + b|$ 能够表示点 x 到距离超平面的远近 ，而通过观察 $\omega^{T} x + b$的符号与类标记 y 的符号是否一致可判断分类是否正确，所以，可以用 $y(\omega^{T} x + b)$ 的正负性来判定或表示分类的正确性。</p><p>给定的训练数据集T和超平面w,b)，定义超平面(w,b)关于样本点(xi,yi)的函数间隔为：$$\hat{\gamma}=y(\omega^{T}x+b)=y(f(x))$$但这样定义的函数间隔有问题，即如果成比例的改变 w 和 b（如将它们改成 2w 和 2b），则函数间隔的值 f(x) 却变成了原来的 2 倍（虽然此时超平面没有改变），所以只 有函数间隔还远远不够。</p><p>平面法向单位化的函数间隔，即几何间隔$$\gamma=\frac{\omega^{T}x+b}{||\omega||}=\frac{f((x)}{||\omega||}$$假定对于一个点 x ，令其垂直投影到超平面上的对应点为 $x_0$ ，w 是垂直于超平面 的一个向量， 为样本 x 到分类间隔的距离，如图所示。<img src="http://img.blog.csdn.net/20170226134808933?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="log"></p><p>从上述的定义可以看出：几何间隔就是函数间隔除以 $∥\omega∥$，而且函 数间隔 $y(w ^T x + b) = yf(x)$ 实际上就是 $|f(x)|$，只是人为定义的一个间隔度量，而几何 间隔 $|f(x)|/∥\omega∥$ 才是直观上的点到超平面的距离。</p><h2>最大间隔分类器</h2><p>对一个数据点进行分类， SVM的思想是当超平面离数据点的“间隔”越大， 分类的确信度 （conﬁdence）也越大。<img src="http://img.blog.csdn.net/20170226135127419?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>定义目标函数：<img src="http://img.blog.csdn.net/20170226135744814?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>回顾一下几何间隔的定义 $\tilde{\gamma}=y\gamma = \frac{hat{\gamma}}{∥\omega∥}$ 可知， 如果令函数间隔 $\hat{\gamma}$等于 1， 则有$\tilde{\gamma}=\frac{1}{∥w∥}$ 且 $y_i (\omega ^T x_i + b) \geq1; \quad i = 1; \cdots; n$</p><p>从而上述目标函数转化成了：</p><p>$$ max\quad \frac{1}{||w||} ;\ s.t. y_i (w^{T} x_i + b)\geq1;\quad i = 1, \cdots,n$$</p><p>这个目标函数便是在相应的约束条件$y_i (w^T x_i + b) \geq1;\quad i = 1, \cdots,n$条件下，最大化这个 $\frac{1}{||w||}$ 值，而 $\frac{1}{||w||}$便是几何间隔$\tilde{\gamma}$。</p><h2>拉格朗日乘子法</h2><p>由于求 $\frac{1}{||w||}$ 的最大值相当于求 $\frac{1}{2}||w||^2$ 的最小值，所以上述目标函数等价于</p><p>$$ min\quad \frac{1}{2}||w||^2 ; \ s.t. y_i (w^T x_i + b) \geq1;\quad i = 1, \cdots,n$$</p><p>因为现在的目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题。这个问题可以用现成的 QP (Quadratic Programming) 优化包进行求解。一言以蔽之：在一定的约束条件下，目标最优，损失最小。</p><p>此外，由于这个问题的特殊结构，还可以通过拉格朗日对偶性（Lagrange Duality） 变换到对偶变量 (dual variable) 的优化问题， 即通过求解与原问题等价的对偶问题 （dual problem）得到原始问题的最优解，这就是线性可分条件下支持向量机的对偶算 法，这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数， 进而推广到非线性分类问题。</p><p>那什么是拉格朗日对偶性呢？简单来讲，通过给每一个约束条件加上一个拉格朗日乘子（Lagrange multiplier），定义拉格朗日函数</p><p>$$L(\omega,b,\alpha)= \frac{1}{2}||w||^2-\sum_{i=1}^{n}\alpha_i(y_i(w_i^Tx_i+b)-1)$$</p><p>原问题是极小极大问题:$$\underset{\omega,b}{Min}\underset{b}{Max}L(\omega,b,\alpha)$$</p><p>原始问题的对偶问题，是极大极小问题:$$\underset{b}{Max}\underset{\omega,b}{Min}L(\omega,b,\alpha)$$</p><p>将拉格朗日函数$L(w,b,\alpha)$分别对w，b求偏导并令其为0,</p><p><img src="http://img.blog.csdn.net/20170226142614206?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="log00"></p><p>将上式带入拉格朗日函数$L(w,b,\alpha)$中，得到：</p><p><img src="http://img.blog.csdn.net/20170226142712754?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="log01"></p><p>继续求$\underset{w,b}{min}L(w,b,\alpha)$对$\alpha$的极大值:</p><p><img src="http://img.blog.csdn.net/20170226142934391?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="log02"></p><p>整理目标函数，求解出最优的$\alpha^{*}$<img src="http://img.blog.csdn.net/20170226172322298?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="log03"></p><p>上式为一般的含不等式约束问题，存在最优化解法的必要和充分条件即KKT条件（详情可查看<a href="http://blog.csdn.net/touristman5/article/details/57418552" target="_blank" rel="noopener">等式约束与不等式约束问题</a>）：为方便理解，我们把所有的不等式约束、等式约束和目标函数全部写为一个式子，简化为$$L(a,b,x)=f(x)+a∗g(x)+b∗h(x)$$</p><p>KKT条件是说最优值必须满足以下条件：</p><ol><li>$\frac{\partial{L}}{\partial{x_i}}=0$对x求导为零；</li><li>$h(x) =0;$</li><li>$a*g(x) = 0;$</li></ol><p>求取这些等式之后就能得到候选最优值。其中第三个式子非常有趣，因为$g(x)&lt;=0$，如果要满足这个等式，必须$\alpha=0$或者$g(x)=0$. 这是SVM的很多重要性质的来源，如支持向量的概念。</p><p>所谓 支撑向量Supporting Vector 也在这里显示出来——事实上，所有非 Supporting Vector 所对应的系数都是等于零的，因此对于新点的内积计算实际上 只要针对少量的“支持向量”而不是所有的训练数据即可。</p><h3>核函数</h3><p>对于线性不可分的情况，可以使用核函数，将输入空间映射到特征空间（通俗说来是从低维空间映射到高维空间），从而使得原本线性不可分的样本可以在特征空间可分。</p><p><img src="http://img.blog.csdn.net/20170226174650918?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="log04"></p><p>在实际应用中，往往依赖先验领域知识才能选择有效的核函数####常见的核函数有</p><ul><li>多项式核函数：$$K(x_1,x_2)=(\left \langle x_1,x_2  \right \rangle)^d$$</li><li>高斯核函数：$$K(x_1,x_2)=exp^ { \frac{||x_1-x_2||}{2\sigma^2} } $$</li></ul><p>参考链接：</p><ol><li>统计学习方法，李航著，清华大学出版社，2012年</li><li><a href="http://blog.csdn.net/v_july_v/article/details/7624837" target="_blank" rel="noopener">http://blog.csdn.net/v_july_v/article/details/7624837</a></li><li><a href="http://www.cnblogs.com/zjgtan/archive/2013/09/03/3298213.html" target="_blank" rel="noopener">http://www.cnblogs.com/zjgtan/archive/2013/09/03/3298213.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;通俗来讲，SVM是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="统计学" scheme="http://yoursite.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>用Tensflow写简单的神经网络</title>
    <link href="http://yoursite.com/2017/02/25/%E7%94%A8Tensflow%E5%86%99%E7%AE%80%E5%8D%95%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2017/02/25/用Tensflow写简单的神经网络/</id>
    <published>2017-02-24T17:40:33.000Z</published>
    <updated>2018-02-23T11:33:21.256Z</updated>
    
    <content type="html"><![CDATA[<p>TensorFlow是一个采用数据流图（data flow graphs），用于数值计算的开源软件库。节点（Nodes）在图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor）。它灵活的架构让你可以在多种平台上展开计算，例如台式计算机中的一个或多个CPU（或GPU），服务器，移动设备等等。</p><a id="more"></a><p><img src="http://tensorfly.cn/images/tensors_flowing.gif" alt="log00"></p><p>根据上图，可以看出一个简单神经网络所具有的模块结构，首先输入层(Input Layer)，接受相关的结构化化数据；其次是隐藏层(Hidden Layer)，隐藏层主要加权运算，通过激活函数达到拟合线性非线性函数的目的；最后有输出层(Output Layer)，其结果成为下一次迭代的初始值。</p><p>一个的单层神经网络如下：<img src="http://hahack.com/images/ann2/w4eQd.png" alt="log01"></p><p>就此，我们用Tensorflow实现一个单层神经网络，参考代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python  </span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-  </span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs,in_size,out_size,activation_function=None)</span>:</span></span><br><span class="line">    Weights = tf.Variable(tf.random_normal([in_size,out_size]))</span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>,out_size])+<span class="number">0.1</span>)</span><br><span class="line">    Wx_plus_b = tf.matmul(inputs,Weights)+biases</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b)</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据</span></span><br><span class="line">x_d = np.linspace(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">300</span>)[:,np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>,<span class="number">0.05</span>,x_d.shape)</span><br><span class="line">y_d = np.square(x_d) - <span class="number">0.5</span> + noise</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义placeholder，可以更方便</span></span><br><span class="line">xs = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">1</span>])</span><br><span class="line">ys = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加隐藏层</span></span><br><span class="line">layer1 = add_layer(xs,<span class="number">1</span>,<span class="number">10</span>,activation_function = tf.nn.relu)</span><br><span class="line">predict = add_layer(layer1,<span class="number">10</span>,<span class="number">1</span>,activation_function = <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - predict),reduction_indices=[<span class="number">1</span>]))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.3</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line"><span class="comment"># init = tf.initialize_all_variables() no long valid from</span></span><br><span class="line"><span class="comment"># 2017-03-02 if using tensorflow &gt;= 0.12</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">1</span>]) &lt; <span class="number">12</span>:</span><br><span class="line">    init = tf.initialize_all_variables()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs: x_d, ys: y_d&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># to see the step improvement</span></span><br><span class="line">        print(sess.run(loss, feed_dict=&#123;xs: x_d, ys: y_d&#125;))</span><br></pre></td></tr></table></figure><p>单机运行结果如下：<img src="http://img.blog.csdn.net/20170225020607154?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="log2"></p><blockquote><p>[1]参考链接：<a href="https://www.youtube.com/watch?v=S9wBMi2B4Ss&amp;list=PLXO45tsB95cKI5AIlf5TxxFPzb-0zeVZ8&amp;index=13" target="_blank" rel="noopener">https://www.youtube.com/watch?v=S9wBMi2B4Ss&amp;list=PLXO45tsB95cKI5AIlf5TxxFPzb-0zeVZ8&amp;index=13</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TensorFlow是一个采用数据流图（data flow graphs），用于数值计算的开源软件库。节点（Nodes）在图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor）。它灵活的架构让你可以在多种平台上展开计算，例如台式计算机中的一个或多个CPU（或GPU），服务器，移动设备等等。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>唐璜节选</title>
    <link href="http://yoursite.com/2017/02/23/%E5%94%90%E7%92%9C%E8%8A%82%E9%80%89/"/>
    <id>http://yoursite.com/2017/02/23/唐璜节选/</id>
    <published>2017-02-22T16:00:00.000Z</published>
    <updated>2018-02-23T10:35:18.038Z</updated>
    
    <content type="html"><![CDATA[<p>   诗歌能做到以更简练、更摄人心魄的精准语言与我们的灵魂直接沟通。</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">生命激荡着青葱的希望</span><br><span class="line">爱意伴随着激情的火焰</span><br><span class="line">美食、酒神，都是爱情的风帆</span><br><span class="line">——唐璜-「七」岩穴奇情</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">生命的美好不能束缚他，死亡的狰狞也不能毁灭它</span><br><span class="line">来自他的母亲—摩尔人赋予她刚强的性格</span><br><span class="line">他将彻底面对世界</span><br><span class="line">乐园和荒漠，没有第三条路</span><br><span class="line"></span><br><span class="line">希腊的少女曾用哀歌，咏叹海黛的爱</span><br><span class="line">迁居的岛民，也曾在漫漫长夜将这一切讲述</span><br><span class="line">夜色大海静，传说流万古</span><br><span class="line">孤岛痴女情，唯有诗人知</span><br><span class="line">——唐璜-「十六」伤逝</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">爱情犹如鸦片，不用太多</span><br><span class="line">一口便让人迷醉</span><br><span class="line">除了泪水情人的眼睛什么都能汲取</span><br><span class="line">尤其是生命的泉水</span><br><span class="line"></span><br><span class="line">这已经足够，爱情虚无飘渺</span><br><span class="line">它因自私而起，又因自私结束</span><br><span class="line">还有一种爱情只是一时的热忱</span><br><span class="line">把自己的脆弱与孤独的美相结合</span><br><span class="line">点缀那一颗疯狂的无法遏制的心</span><br><span class="line">如果没有这种美，热情也就消失</span><br><span class="line">——唐璜-「三十二」女皇的恩宠</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;   诗歌能做到以更简练、更摄人心魄的精准语言与我们的灵魂直接沟通。&lt;/p&gt;
    
    </summary>
    
    
      <category term="文字" scheme="http://yoursite.com/tags/%E6%96%87%E5%AD%97/"/>
    
  </entry>
  
  <entry>
    <title>共轭先验</title>
    <link href="http://yoursite.com/2017/02/18/%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/"/>
    <id>http://yoursite.com/2017/02/18/共轭先验/</id>
    <published>2017-02-17T16:00:00.000Z</published>
    <updated>2018-02-23T17:55:38.679Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>定义：如果先验分布和似然函数可以使得先验分布和后验分布有相同的形式，那么就称先验分布与似然函数是共轭的。</p></blockquote><a id="more"></a><p>读数理统计学导论时，遇到过共轭先验的概念。  贝叶斯判别准则中，分别假设了先验分布$p(\theta)$，后验分布$p(\theta|X)$，以及$p(X), p(X|\theta)$似然函数。</p><p>贝叶斯定理可以写作：$$P(\theta|X)=\frac{P(\theta)P(X|\theta)}{P(X)}$$</p><p>即 「后验分布 =先验分布 * 似然函数 / P(X)」</p><p>  之所以采用共轭先验的原因是可以使得先验分布和后验分布的形式相同，这样一方面合符人的直观（它们应该是相同形式的；另外一方面是可以形成一个先验链，即现在的后验分布可以作为下一次计算的先验分布，如果形式相同就可以形成一个链条。为了使得先验分布和后验分布的形式相同，我们定义：如果先验分布和似然函数可以使得先验分布和后验分布有相同的形式，那么就称先验分布与似然函数是共轭的。所以共轭是指：先验分布和似然函数共轭。</p><h3>例子：</h3><p>共轭先验通常可以由分布的pdf或pmf来确定。</p><p>考虑二项模型：$$p(x)={n \choose x}q^{x}(1-q)^{n-x}$$</p><p>写成以q为参数的函数形式：$$f(q)\propto q^{a}(1-q)^{b}$$</p><p>通常这个函数应该还缺少一个乘数因子，以保证pdf的积分值为1。</p><p>这个乘数项是a，b的函数。写作下面的形式$$p(q)={q^{\alpha -1}(1-q)^{\beta -1} \over \mathrm {B} (\alpha ,\beta )}$$</p><p>可以看出，乘上的$\mathrm {B} (\alpha ,\beta )$作为归一化常熟存在，根据上面定义，可得二项分布的共轭分布族是<strong>贝塔分布。</strong></p><p>  与共轭先验对应的概念是共轭分布族(Conjugate family of distribution),所谓共轭分布族是指参数$\theta$的后验pdf与作为先验的分布族是相同的，则称此类先验pdf关于具有pdf$f(x|\theta),\theta \in \Omega$的分布族为共轭分布。</p><p>  例如，给定$\theta$时随机变量X的pmf是均值为$\theta$的泊松分布。若我们选取伽马先验，由贝叶斯定理计算出后验也是伽马分布族。则称，伽马分布族构成这种泊松模型的共轭先验类。</p><p>参考资料：</p><ol><li>Pattern Recognition and Machine Learning ,  M. Bishop</li><li>数理统计学导论 ,Robert V.Hogg</li><li>Conjugate prior - Wikipedia</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;定义：如果先验分布和似然函数可以使得先验分布和后验分布有相同的形式，那么就称先验分布与似然函数是共轭的。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Mathematic" scheme="http://yoursite.com/tags/Mathematic/"/>
    
      <category term="统计学" scheme="http://yoursite.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>全局优化算法之粒子群算法</title>
    <link href="http://yoursite.com/2017/02/05/%E5%85%A8%E5%B1%80%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E4%B9%8B%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2017/02/05/全局搜索算法之粒子群算法/</id>
    <published>2017-02-04T16:00:00.000Z</published>
    <updated>2018-02-24T06:03:43.497Z</updated>
    
    <content type="html"><![CDATA[<h2>序言</h2><p>  前面讨论过一些迭代算法，包括牛顿法、梯度方法、共轭梯度方法和拟牛顿法，能够从初始点出发，产生一个迭代序列。很多时候，迭代序列只能收敛到局部极小点。因此，为了保证算法收敛到全局最小点，有时需要在全局极小点附近选择初始点。此外，这些方法需要计算目标函数。</p><p>  全局优化算法又称现代启发式算法，是一种具有全局优化性能、通用性强且适合于并行处理的算法。这种算法一般具有严密的理论依据，而不是单纯凭借专家经验，理论上可以在一定的时间内找到最优解或近似最优解。遗传算法属于智能优化算法之一。</p><p>  常用的全局优化算法有： 遗传算法 、模拟退火算法、禁忌搜索算法、粒子群算法、蚁群算法。</p><h2>PSO算法</h2><p>  粒子群算法（Particle swarm optimization）是由James Kennedy &amp;<a href="http://www.engr.iupui.edu/~eberhart/" target="_blank" rel="noopener">Russell Eberhart</a>提出。区别于上节讨论的模拟退火算法，粒子群算法并不是只更新单个迭代点，而是更新一组迭代点，称为群。群中每个点称为粒子。可将群视为一个无序的群体，其中的每个成员都在移动，意在形成聚集，但移动方向是随机的。</p><p>  具体来说，求取目标函数在$\mathbb{R}^n$上极小点的过程。</p><ol><li>在$\mathbb{R}^n$随机产生一组数据点，为每个点赋予一个速度，构成一个速度向量。这些点视为粒子所在的位置，以指定速度在运动。</li><li>针对每个数据点计算对应的目标函数值，基于计算结果，产生一组新的数据点，赋予新的运动速度。</li></ol><p>  其每个粒子都持续追踪到目前为止最好的位置，称到目前为止最好的位置为Pbest，全局最好为止Gbest。基于粒子的个体最好位置和群的群的全局最优位置，调整各粒子的运动速度，实现粒子的“交互“。即在每次迭代中，产生两个随机数，分别作为pbest和gbest的权重，以此构成pbest和gbest的一个组合值，分别称为速度项和随机项，再加上加权后的原有速度，可以实现对原有速度的更新。</p><p>  目标函数在$\mathbb{R}^n$中，由种群数m组成粒子群，其中第i个粒子在d维的位置为$x_{id}$，其飞行速度为$v_{id}$，该粒子当前搜索的最优位置为，整个粒子群当前位置为，更新公式如下：$$v_{id}^{t+1}=v_{id}^{t}+c_1r_1(P_{id}-x_{id}+c_2r_2(P_{gd}-x_{id}))\quad (1) \ x_{id}^{t+1}=x_{id}{t}+v_{id}^{t+1} \quad \quad(2)$$</p><p>$r_1、r_2$是服从U(0,1)分布的随机数，学习因子$c_1、c_2$为非负常数，通常取$c_1=c_2=2$，$v_id \in [v_{min},v_{max}],v_{max}$是自设定的常数，迭代终止条件为预设的最大迭代数或预定的最小适应度阈值。</p><p>Matlab代码示例：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% 该代码为基于PSO的函数极值寻优</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">%% 清空环境</span></span><br><span class="line">clc</span><br><span class="line">clear</span><br><span class="line"></span><br><span class="line"><span class="comment">%% 参数初始化</span></span><br><span class="line"><span class="comment">%粒子群算法中的两个参数</span></span><br><span class="line">c1 = <span class="number">1.49445</span>;</span><br><span class="line">c2 = <span class="number">1.49445</span>;</span><br><span class="line"></span><br><span class="line">maxgen=<span class="number">500</span>;   <span class="comment">% 进化次数  </span></span><br><span class="line">sizepop=<span class="number">100</span>;   <span class="comment">%种群规模</span></span><br><span class="line"></span><br><span class="line">Vmax=<span class="number">1</span>;</span><br><span class="line">Vmin=<span class="number">-1</span>;</span><br><span class="line">popmax=<span class="number">5</span>;</span><br><span class="line">popmin=<span class="number">-5</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">%% 产生初始粒子和速度</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:sizepop</span><br><span class="line">    <span class="comment">%随机产生一个种群</span></span><br><span class="line">    pop(<span class="built_in">i</span>,:)=<span class="number">5</span>*rands(<span class="number">1</span>,<span class="number">2</span>);    <span class="comment">%初始种群</span></span><br><span class="line">    V(<span class="built_in">i</span>,:)=rands(<span class="number">1</span>,<span class="number">2</span>);  <span class="comment">%初始化速度</span></span><br><span class="line">    <span class="comment">%计算适应度</span></span><br><span class="line">    fitness(<span class="built_in">i</span>)=fun(pop(<span class="built_in">i</span>,:));   <span class="comment">%染色体的适应度</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 个体极值和群体极值</span></span><br><span class="line">[bestfitness bestindex]=min(fitness);</span><br><span class="line">zbest=pop(bestindex,:);   <span class="comment">%全局最佳</span></span><br><span class="line">gbest=pop;    <span class="comment">%个体最佳</span></span><br><span class="line">fitnessgbest=fitness;   <span class="comment">%个体最佳适应度值</span></span><br><span class="line">fitnesszbest=bestfitness;   <span class="comment">%全局最佳适应度值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 迭代寻优</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:maxgen</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span>=<span class="number">1</span>:sizepop</span><br><span class="line">        </span><br><span class="line">        <span class="comment">%速度更新</span></span><br><span class="line">        V(<span class="built_in">j</span>,:) = V(<span class="built_in">j</span>,:) + c1*<span class="built_in">rand</span>*(gbest(<span class="built_in">j</span>,:) - pop(<span class="built_in">j</span>,:)) + c2*<span class="built_in">rand</span>*(zbest - pop(<span class="built_in">j</span>,:));</span><br><span class="line">        V(<span class="built_in">j</span>,<span class="built_in">find</span>(V(<span class="built_in">j</span>,:)&gt;Vmax))=Vmax;</span><br><span class="line">        V(<span class="built_in">j</span>,<span class="built_in">find</span>(V(<span class="built_in">j</span>,:)&lt;Vmin))=Vmin;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">%种群更新</span></span><br><span class="line">        pop(<span class="built_in">j</span>,:)=pop(<span class="built_in">j</span>,:)+<span class="number">0.5</span>*V(<span class="built_in">j</span>,:);</span><br><span class="line">        pop(<span class="built_in">j</span>,<span class="built_in">find</span>(pop(<span class="built_in">j</span>,:)&gt;popmax))=popmax;</span><br><span class="line">        pop(<span class="built_in">j</span>,<span class="built_in">find</span>(pop(<span class="built_in">j</span>,:)&lt;popmin))=popmin;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">%适应度值</span></span><br><span class="line">        fitness(<span class="built_in">j</span>)=fun(pop(<span class="built_in">j</span>,:)); </span><br><span class="line">   </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span>=<span class="number">1</span>:sizepop</span><br><span class="line">        </span><br><span class="line">        <span class="comment">%个体最优更新</span></span><br><span class="line">        <span class="keyword">if</span> fitness(<span class="built_in">j</span>) &lt; fitnessgbest(<span class="built_in">j</span>)</span><br><span class="line">            gbest(<span class="built_in">j</span>,:) = pop(<span class="built_in">j</span>,:);</span><br><span class="line">            fitnessgbest(<span class="built_in">j</span>) = fitness(<span class="built_in">j</span>);</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">%群体最优更新</span></span><br><span class="line">        <span class="keyword">if</span> fitness(<span class="built_in">j</span>) &lt; fitnesszbest</span><br><span class="line">            zbest = pop(<span class="built_in">j</span>,:);</span><br><span class="line">            fitnesszbest = fitness(<span class="built_in">j</span>);</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span> </span><br><span class="line">    yy(<span class="built_in">i</span>)=fitnesszbest;    </span><br><span class="line">        </span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="comment">%% 结果分析</span></span><br><span class="line">plot(yy)</span><br><span class="line">title(<span class="string">'最优个体适应度'</span>,<span class="string">'fontsize'</span>,<span class="number">12</span>);</span><br><span class="line">xlabel(<span class="string">'进化代数'</span>,<span class="string">'fontsize'</span>,<span class="number">12</span>);ylabel(<span class="string">'适应度'</span>,<span class="string">'fontsize'</span>,<span class="number">12</span>);</span><br></pre></td></tr></table></figure><p>参考论文：</p><ol><li>An introduction to optimization-最优化导论[J]. Edwin K.P.Chong.</li><li><a href="http://www.jos.org.cn/1000-9825/18/861.pdf" target="_blank" rel="noopener">一种更简化更高效的粒子群算法</a></li></ol>]]></content>
    
    <summary type="html">
    
      全局优化算法又称现代启发式算法，是一种具有全局优化性能、通用性强且适合于并行处理的算法。这种算法一般具有严密的理论依据，而不是单纯凭借专家经验，理论上可以在一定的时间内找到最优解或近似最优解。
    
    </summary>
    
    
      <category term="优化算法" scheme="http://yoursite.com/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>全局优化算法之遗传算法</title>
    <link href="http://yoursite.com/2017/02/03/%E5%85%A8%E5%B1%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E4%B9%8B%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2017/02/03/全局优化算法之遗传算法/</id>
    <published>2017-02-02T16:00:00.000Z</published>
    <updated>2018-02-24T06:23:39.363Z</updated>
    
    <content type="html"><![CDATA[<h2>全局优化算法概述</h2><p>前面讨论过一些迭代算法，包括牛顿法、梯度方法、共轭梯度方法和拟牛顿法，能够从初始点出发，产生一个迭代序列。很多时候，<font color="blue">迭代序列只能收敛到局部极小点。</font>因此，为了保证算法收敛到全局最小点，有时需要在全局极小点附近选择初始点。此外，这些方法需要计算目标函数。</p><a id="more"></a><p>全局优化算法又称现代启发式算法，是一种具有全局优化性能、通用性强且适合于并行处理的算法。这种算法一般具有严密的理论依据，而不是单纯凭借专家经验，理论上可以在一定的时间内找到最优解或近似最优解。遗传算法属于智能优化算法之一。</p><p>常用的全局优化算法有：遗传算法 、模拟退火算法、禁忌搜索算法、粒子群算法、蚁群算法。</p><h3>1、染色体编码</h3><p>实际上遗传算法并不是直接针对约束集中的点进行操作，而是针对这些点的编码后再进行相关变异交叉等操作。具体说来，如约束集$\omega$中的点24映射为一个字符串集合-- 11000，这些字符串全部都是等长的，称为染色体。基本遗传算法（SGA）使用二进制串进行编码。</p><h3>2、适应度函数</h3><p>遗传算法对一个个体（解）的好坏用适应度函数值来评价，适应度函数值越大，解的质量越好。适应度函数是遗传算法进化过程的驱动力，也是进行自然选择的唯一标准，它的设计应结合求解问题本身的要求而定。</p><h3>3、选择和进化步骤</h3><p>在选择步骤中，利用选择操作构造一个新的种群$M(k)$，使其个体数量与种群相等，种群中个体数量称为个体容量，用N表示，M是在P的基础上进行随机处理后得到的，即M中的每个个体以概率<font size="3">$$\frac{f(x^{(k)})}{F(k)}$$</font>等于$P(k)$中的$x^{(k)}$，其中，<font size="3">$F(k)=\sum f(x_{i}^{(k)})$</font>，指的是对整个P进行求和，也就是说，染色体被选中的概率与其适应度函数值大小成正比。</p><h4>轮盘赌选择方法：</h4><img src="http://hi.csdn.net/attachment/201101/12/8394323_1294826341f6bF.jpg" width="50%" height="50%"><p>轮盘赌选择法可用如下过程模拟来实现：</p><ol><li>在［0, 1］内产生一个均匀分布的随机数r。</li><li>若r≤q(1),则染色体x(1)被选中。</li><li>若$q(k-1)&lt; r ≤ q(k)$,其中(2≤k≤N), 则染色体x(k)被选中。其中的qi称为染色体$x_i (i=1, 2, \cdots, n)$的积累概率, 其计算公式为</li></ol><p>$$q_i=\sum_{j=1}^{i}P_j$$</p><p>得到积累概率为：</p><img src="http://hi.csdn.net/attachment/201101/12/8394323_129482634014NA.jpg" width="50%" height="50%"><p>轮盘赌选择方法的实现步骤:</p><ol><li>计算群体中所有个体的适应度值；</li><li>计算每个个体的选择概率；</li><li>计算积累概率；</li><li>采用模拟赌盘操作（即生成0到1之间的随机数与每个个体遗传到下一代群体的概率进行匹配）来确定各个个体是否遗传到下一代群体中。</li></ol><h3>4、交叉算子</h3><p>交叉运算，是指对两个相互配对的染色体依据交叉概率，按某种方式相互交换其部分基因，从而形成两个新的个体。</p><p>交叉运算是遗传算法区别于其他进化算法的重要特征，它在遗传算法中起关键作用，是产生新个体的主要方法。基本遗传算法（SGA）中交叉算子采用单点交叉算子。</p><p>单点交叉运算</p><p><img src="http://hi.csdn.net/attachment/201101/12/8394323_1294826338NWUh.jpg" alt="log02"></p><h3>5、变异算子</h3><ul><li>变异运算，是指改变个体编码串中的某些基因值，从而形成新的个体。</li><li>变异运算是产生新个体的<strong>辅助方法</strong>，决定遗传算法的局部搜索能力，保持种群多样性。</li><li>交叉运算和变异运算的相互配合，共同完成对搜索空间的全局搜索和局部搜索。</li><li>基本遗传算法（SGA）中变异算子采用基本位变异算子。</li></ul><p>基本位变异算子是指对个体编码串随机指定的某一位或某几位基因作变异运算。对于二进制编码符号串所表示的个体，若需要进行变异操作的某一基因座上的原有基因值为0，则将其变为1；反之，若原有基因值为1，则将其变为0 。</p><p><img src="http://hi.csdn.net/attachment/201101/12/8394323_129482633872s3.jpg" alt="log03"></p><div align="center">基本位变异算子的执行过程</div><p><font color="red"><strong>交叉和变异操作目的在于创建一个新的种群，使得新种群目标函数的平均值能够大于上一代种群。总的说来，遗传算法就是针对种群迭代开展交叉和变异操作，产生新种群，直到满足预定的停止条件。</strong></font></p><p>Matlab示例：</p><p>选择适应度函数为：$f(x) = x + 10sin(5x) + 7cos(4x)$函数图像为</p><p><img src="http://img.blog.csdn.net/20170227193922105?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="log04"></p><p>运行结果为：</p><p><img src="http://img.blog.csdn.net/20170227193901589?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="log05"></p><p>最优个体为： 10101111011111011</p><p>最优值为：24.8554</p><p>相关Matlab代码参考：<a href="https://github.com/yanshengjia/artificial-intelligence/tree/master/genetic-algorithm-for-functional-maximum-problem/src" target="_blank" rel="noopener">Github地址</a></p><ol><li>An introduction to optimization-最优化导论[J]. Edwin K.P.Chong.</li><li><a href="http://blog.csdn.net/v_JULY_v/article/details/6132775" target="_blank" rel="noopener">http://blog.csdn.net/v_JULY_v/article/details/6132775</a></li></ol>]]></content>
    
    <summary type="html">
    
      全局优化算法又称现代启发式算法，是一种具有全局优化性能、通用性强且适合于并行处理的算法。这种算法一般具有严密的理论依据，而不是单纯凭借专家经验，理论上可以在一定的时间内找到最优解或近似最优解。
    
    </summary>
    
    
      <category term="优化算法" scheme="http://yoursite.com/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>全局优化算法之模拟退火算法</title>
    <link href="http://yoursite.com/2017/02/03/%E5%85%A8%E5%B1%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E4%B9%8B%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2017/02/03/全局优化算法之模拟退火算法/</id>
    <published>2017-02-02T16:00:00.000Z</published>
    <updated>2018-02-23T17:59:28.415Z</updated>
    
    <content type="html"><![CDATA[<h2>序言</h2><p>前面讨论过一些迭代算法，包括牛顿法、梯度方法、共轭梯度方法和拟牛顿法，能够从初始点出发，产生一个迭代序列。很多时候，迭代序列只能收敛到局部极小点。因此，为了保证算法收敛到全局最小点，有时需要在全局极小点附近选择初始点。此外，这些方法需要计算目标函数。</p><a id="more"></a><p>全局优化算法又称现代启发式算法，是一种具有全局优化性能、通用性强且适合于并行处理的算法。这种算法一般具有严密的理论依据，而不是单纯凭借专家经验，理论上可以在一定的时间内找到最优解或近似最优解。遗传算法属于智能优化算法之一。</p><p>常用的全局优化算法有：遗传算法 、模拟退火算法、禁忌搜索算法、粒子群算法、蚁群算法。</p><h2>1、随机搜索算法</h2><p>模拟退火算法是一种随机搜索算法，随机搜索方法也称作概率搜索算法，这很好理解，是一种能够在优化问题的可行集中随机采样，逐步完成搜索的算法。German首次将模拟退火算法应用在凸显处理领域。<a href="http://www.stat.cmu.edu/~acthomas/724/Geman.pdf" target="_blank" rel="noopener">论文地址</a>后续有时间我可以是这翻译一下。</p><h3>朴素随机搜索算法步骤：</h3><ol><li>令$K=0$，选定初始点$x^{(0)}\in \Omega$</li><li>从$N(x^{(k)})$中随机选定一个备选点$z^{(k)}$</li><li>如果$f(z^{(k)}) &lt; f(x^{(k)})$,则令$x^{(k+1)}=z_{(k)}$，否则$x^{(k+1)}=x_{(k)}$</li><li>如果满足停止条件，则停止迭代</li><li>令$k=k+1$，回到第2步</li></ol><p>算法分析：朴素随机搜索算法面临的问题在于领域$N(x^{(k)})$的设计，一方面要保证领域足够大，否则算法可能会在局部点&quot;卡住&quot;；但如果使领域太大的话，会使得搜索过程变得很慢。另一种，对领域问题的解决方案是对朴素随机搜索算法进行修改，使其能够&quot;爬出&quot;局部极小点的&quot;领域&quot;。<font color="red"><strong>这意味着两次迭代中，算法产生的新点可能会比当前点要差。模拟退火算法就设计了这样的机制。</strong></font></p><h2>2、模拟退火算法</h2><h3>算法步骤</h3><ol><li>令$K=0$，选定初始点$x^{(0)}\in \Omega$</li><li>从$N(x^{(k)})$中随机选定一个备选点$z^{(k)}$</li><li><font color="red"><strong>设计一枚特殊的硬币，使其在一次抛投过程中出现正面的概率为$P(k,f(z^{(k)}),f(x^{(k)}))$。抛一次硬币，如果出现正面，则令$x^{(k+1)}=z^{(k)}$，否则$x^{(k+1)}=x_{(k)}。$</strong></font></li><li>如果满足停止条件，则停止迭代</li><li>令$k=k+1$，回到第2步</li></ol><p>注：其中所说的&quot;抛硬币&quot;实际可理解成一种随机决策。</p><p>算法进行中，第k次迭代，可以追踪到目前最好的点$x_{best}^{(k)}$，即能够对所有的$i \in {0,\cdots ,k },$都有$f(x^{(j)})\leqslant  f(x^{(i)})$成立的$x^{(j)}$。</p><p>$x_{best}^{(k)}$按照以下方式进行更新</p><p><img src="http://img.blog.csdn.net/20170228102458217?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p><font color="red"><strong>通过持续追踪并更新当前为止最好的点，可以将模拟退火算法简单视为一个搜索过程，搜索过程的最终目的是出处当前为止最好的点。这种说法适合绝大部分启发式算法。</strong></font></p><h2>3、模拟退火算法与朴素随机搜索算法的区别</h2><p>模拟退火算法与朴素随机搜索算法区别在于步骤3，该步骤中，<font color="red"><strong>模拟退火算法以一定的概率选择备选点作为下一次迭代点，即使这个备选点比当前的迭代点要差。这一概率被称作接受概率，接受概率要合理设定，才能保证迭代过程正确进行</strong>。</font>$$P(k,f(z^{(k)}),f(x^{(k)}))=min(1,exp(\frac{-f(x^{(k)})+f(z^{(k)})}{T_k}))$$$T_k$称为冷却温度</p><p>从上式我们至少可以推出，如果$f(z^{(k)})\leqslant f(x^{(k)})$，则p=1，即$x^{(k+1)}=z^{(k)}$。如果$f(z^{(k)}) &gt;f(x^{(k)})$，则仍有一定概率使得$x^{(k+1)}=z^{(k)}$，这一概率为,$exp(\frac{-f(x^{(k)})+f(z^{(k)})}{T_k})$。</p><p><font color="red"><strong>$f(z^{(k)}) 与f(x^{(k)})$之间差异越大，采用$z^{(k)}$作为下一迭代点的概率就越小。类似的，$T_k$越小，采用$z^{(k)}$作为下一迭代点的概率就越小。</strong></font>通常的做法是令温度$T_k$递减到0（表示冷却过程）。也就是说，随着迭代次数的增加，算法趋于更差点的概率越来越小。</p><p>对于温度参数的研究，可以<a href="https://stuff.mit.edu/afs/athena/course/6/6.435/www/Hajek88.pdf" target="_blank" rel="noopener">参考论文</a></p><h2>4、 模拟退火算法伪代码</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* J(y)：在状态y时的评价函数值</span></span><br><span class="line"><span class="comment">* Y(i)：表示当前状态</span></span><br><span class="line"><span class="comment">* Y(i+1)：表示新的状态</span></span><br><span class="line"><span class="comment">* r： 用于控制降温的快慢</span></span><br><span class="line"><span class="comment">* T： 系统的温度，系统初始应该要处于一个高温的状态</span></span><br><span class="line"><span class="comment">* T_min ：温度的下限，若温度T达到T_min，则停止搜索</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">while</span>( T &gt; T_min )</span><br><span class="line">&#123;</span><br><span class="line">　　dE = J( Y(i+<span class="number">1</span>) ) - J( Y(i) ) ; </span><br><span class="line"></span><br><span class="line">　　<span class="keyword">if</span> ( dE &gt;=<span class="number">0</span> ) <span class="comment">//表达移动后得到更优解，则总是接受移动</span></span><br><span class="line">Y(i+<span class="number">1</span>) = Y(i) ; <span class="comment">//接受从Y(i)到Y(i+1)的移动</span></span><br><span class="line">　　<span class="keyword">else</span></span><br><span class="line">　　&#123;</span><br><span class="line"><span class="comment">// 函数exp( dE/T )的取值范围是(0,1) ，dE/T越大，则exp( dE/T )也</span></span><br><span class="line"><span class="keyword">if</span> ( <span class="built_in">exp</span>( dE/T ) &gt; random( <span class="number">0</span> , <span class="number">1</span> ) )</span><br><span class="line">Y(i+<span class="number">1</span>) = Y(i) ; <span class="comment">//接受从Y(i)到Y(i+1)的移动</span></span><br><span class="line">　　&#125;</span><br><span class="line">　　T = r * T ; <span class="comment">//降温退火 ，0&lt;r&lt;1 。r越大，降温越慢；r越小，降温越快</span></span><br><span class="line">　　<span class="comment">/*</span></span><br><span class="line"><span class="comment">　　* 若r过大，则搜索到全局最优解的可能会较高，但搜索的过程也就较长。若r过小，则搜索的过程会很快，但最终可能会达到一个局部最优值</span></span><br><span class="line"><span class="comment">　　*/</span></span><br><span class="line">　　i ++ ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>参考链接：</p><ol><li>An introduction to optimization-最优化导论[J]. Edwin K.P.Chong.</li><li><a href="http://www.cnblogs.com/heaad/archive/2010/12/20/1911614.html" target="_blank" rel="noopener">http://www.cnblogs.com/heaad/archive/2010/12/20/1911614.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      全局优化算法又称现代启发式算法，是一种具有全局优化性能、通用性强且适合于并行处理的算法。这种算法一般具有严密的理论依据，而不是单纯凭借专家经验，理论上可以在一定的时间内找到最优解或近似最优解。
    
    </summary>
    
    
      <category term="优化算法" scheme="http://yoursite.com/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>等式约束与不等式约束问题</title>
    <link href="http://yoursite.com/2017/01/26/%E7%AD%89%E5%BC%8F%E7%BA%A6%E6%9D%9F%E4%B8%8E%E4%B8%8D%E7%AD%89%E5%BC%8F%E7%BA%A6%E6%9D%9F%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2017/01/26/等式约束与不等式约束问题/</id>
    <published>2017-01-25T16:00:00.000Z</published>
    <updated>2018-02-23T11:54:39.291Z</updated>
    
    <content type="html"><![CDATA[<p>针对特殊约束条件下的优化问题，有着不同类别适应不同条件的求解算法。包括梯度法、求解线性等式约束问题的投影梯度法、适用于含有等式约束规划和含有不等式规划的拉格朗日乘子法、针对不等式约束的KKT条件法、罚函数法等。</p><a id="more"></a><h2>等式约束问题</h2><p>设目标函数为f(x)，约束条件为$h_k(x)$，形如$$min \quad f(x) \  s.t. \quad h_k(x)=0 \quad k=1,2,\cdots k$$则解决方法是消元法或者拉格朗日法。消元法不再多说，拉格朗日法这里在提一下，因为后面提到的KKT条件是对拉格朗日乘子法的一种泛化。</p><p>$$L(x,\lambda)=f(x)+\sum_{k=1}^{l}\lambda_kh_k(x)$$其中$λ_k$是各个约束条件的待定系数。然后解偏导方程组：$$\frac{\partial F }{\partial x_i}=0 \quad  \frac{\partial F }{\partial \lambda_k}=0 \ \cdots$$</p><p>至于为什么这么做可以求解最优化？维基百科上给出了一个比较好的直观解释。</p><p>举个二维最优化的例子：</p><p>$$min f(x,y)   \s.t. g(x,y) = c$$</p><p>这里画出$z=f(x,y)$的等高线：<img src="https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1364139753,865873880&amp;fm=27&amp;gp=0.jpg" alt="log01"></p><p>绿线标出的是约束$g(x,y)=c$的点的轨迹。蓝线是$f(x,y)$的等高线。箭头表示斜率，和等高线的法线平行。从梯度的方向上来看，显然有$d1&gt;d2$。绿色的线是约束，也就是说，只要正好落在这条绿线上的点才可能是满足要求的点。如果没有这条约束，$f(x,y)$的最小值应该会落在最小那圈等高线内部的某一点上。<font color="red">而现在加上了约束，最小值点应该在哪里呢？</font>显然应该是在f(x,y)的等高线正好和约束线相切的位置，因为如果只是相交意味着肯定还存在其它的等高线在该条等高线的内部或者外部，使得新的等高线与目标函数的交点的值更大或者更小，<font color="red">只有到等高线与目标函数的曲线相切的时候，可能取得最优值。</font></p><p>如果我们对约束也求梯度$∇g(x,y)$，则其梯度如图中绿色箭头所示。很容易看出来，要想让目标函数$f(x,y)$的等高线和约束相切，则他们切点的梯度一定在一条直线上。即：$\partial f(x,y)=\lambda（\partial g(x,y)-C) $其中λ可以是任何非0实数。</p><p>一旦求出$\lambda$的值，将其带入下式，易求在无约束极值和极值所对应的点。</p><p>$$F(x,y)=f(x,y)+\lambda(g(x,y)-c)$$</p><p>这就是拉格朗日函数的由来。</p><h2>不等式约束问题</h2><p>考虑一般形式的优化问题：$$Min\quad f(x) \ s.t. \quad h(x)=0 \ \quad g(x) \geq 0$$</p><p>由上式，对于一个不等式约束$g_j(x)\leqslant 0$，如果在$x^{<em>}$处$g_j(x)= 0$，那么称该不等式约束是$x^{</em>}$处的起作用约束；如果在$x^{*}$ 处 $g_j(x)\geq 0$，那么称该约束是处的不起作用约束。按惯例，把等式约束$h_i(x)=0$当作总是起作用的约束。</p><p>由此，定义不等式约束下的拉格朗日函数L，则L表达式为：$$L(X,\lambda,\mu)=f(X)+\sum_{j==1}^{p}\lambda_jh_j(X)+\sum_{k=1}^{q}\mu_kg_k(X)$$</p><p>其中f(x)是原目标函数，$h_j(x)$是第j个等式约束条件，$\lambda _j$是对应的约束系数，$g_k$是不等式约束，$\mu_k$是对应的约束系数。</p><p>常用的方法是KKT条件，同样地，把所有的不等式约束、等式约束和目标函数全部写为一个式子，简化为$L(a, b, x)= f(x) + a<em>g(x)+b</em>h(x)$</p><p>KKT条件是说最优值必须满足以下条件：</p><ol><li>$\frac{\partial L }{\partial x_i}=0$对x求导为零；</li><li>h(x) =0;</li><li>a*g(x) = 0;</li></ol><p>求取这些等式之后就能得到候选最优值。其中第三个式子非常有趣，因为$g(x)&lt;=0$，如果要满足这个等式，必须a=0或者$g(x)=0$. 这是SVM的很多重要性质的来源，如支持向量的概念。</p><h3>KKT的推导：</h3><p>首先不加证明的给出对偶问题结论：$$\underset{\omega,b}{Min}\underset{b}{Max}L(\omega,b,\alpha)=\underset{b}{Max}\underset{\omega,b}{Min}L(\omega,b,\alpha)$$</p><p><img src="http://images2015.cnblogs.com/blog/520787/201509/520787-20150901140604513-1723209246.jpg" alt="log02"></p><p>参考资料：</p><ol><li>Edwin K.P.Chong  and Stanisslaw H.Zak 最优化导论（第四版）</li><li><a href="http://blog.csdn.net/xianlingmao/article/details/7919597" target="_blank" rel="noopener">http://blog.csdn.net/xianlingmao/article/details/7919597</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;针对特殊约束条件下的优化问题，有着不同类别适应不同条件的求解算法。包括梯度法、求解线性等式约束问题的投影梯度法、适用于含有等式约束规划和含有不等式规划的拉格朗日乘子法、针对不等式约束的KKT条件法、罚函数法等。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Mathematic" scheme="http://yoursite.com/tags/Mathematic/"/>
    
      <category term="优化算法" scheme="http://yoursite.com/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>广义线性模型(Generalized Linear Model)</title>
    <link href="http://yoursite.com/2017/01/26/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B(Generalized%20Linear%20Model)/"/>
    <id>http://yoursite.com/2017/01/26/广义线性模型(Generalized Linear Model)/</id>
    <published>2017-01-25T16:00:00.000Z</published>
    <updated>2018-02-23T17:59:33.014Z</updated>
    
    <content type="html"><![CDATA[<h2>广义线性模型(Generalized Linear Model)</h2><p>本文沿接接着上节的指数分布族,文章中注了引入指数分布族的概念是为了说明广义线性模型。</p><h2>概念</h2><p>广义线性模型（generalized linear model, GLM)是简单最小二乘回归（OLS)的扩展,在广义线性模式中，假设每个变量的观测值 Y来自某个指数族分布。 该分布的平均数$\mu$可由与该点独立的X解释：$$E(y)=\mu=g(\theta^Tx)$$其中E(y)为y的期望值，$\theta^T x$是由未知待估计参数$\theta$与已知变数X构成的线性估计式，g则为链接函数。在此模式下,y的方差V可表示为：$$Var(y)=V(y)=V(g(\theta ^Tx))$$一般假设V可视为一指数族随机变数的函数。未知参数$\theta$通常会以最大似然、贝叶斯方法估计。</p><h2>例证</h2><p><img src="http://img.blog.csdn.net/20170226105428730?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="log01"><img src="http://img.blog.csdn.net/20170226105451339?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="log02"></p><p>参考此例：$\eta$与伯努利分布中的参数$\varphi$的关系是Logistic函数，再通过推导可以得到Logistic回归。见下文推导示例。</p><p>通过此例，我们可以推想，$\eta$以不同的映射函数与其他概率分布函数中的参数发生联系，从而得到不同的模型，广义线性模型正是将指数族分布中的所有成员都作为线性模型的扩展，通过非线性的连接函数映射到其他空间从而大大扩大了线性模型可解决的问题。</p><h3>假设条件</h3><p>下面我们看看GLM的形式话定义，GLM的三个假设：</p><ol><li>$y|x;\theta～ExpFamily(\eta)$：给定样本x与参数$\theta$,样本分类y服从指数分布族中的某个分布</li><li>给定一个x，我们需要的目标函数为$h_\theta(x)=E\left[ T(y)|x\right]$</li><li>$\eta=\theta^Tx$</li></ol><h3>上例推导</h3><p>依据三个假设，我们可以推导出logistic模型与最小二乘模型。Logistic模型的推导过程如下：$$h_\theta(x)=E\left[ T(y)|x\right] =E\left[ y|x\right]=\mu=\eta=\theta^Tx$$</p><p>其中，将$\eta$与原始概率分布中的参数联系起来的函数成为正则相应函数，如$\varphi=\frac{1}{1+e^(-\eta)},\mu=\eta$即是正则响应函数。正则响应函数的逆称为正则关联函数。</p><p>所以，对于广义线性模型，需要决策的是选用什么样的分布，当选取高斯分布时，我们可以得到最小二乘模型，当选取伯努利分布时，我们得到logistic模型，这里所说的模型是假设函数h的形式。</p><p>同样，可以将Logistic函数做拉伸变换，可以得到新的连接函数</p><p>$$\varphi=\frac{1}{1+e^{-\lambda\eta}}$$</p><h3>总结</h3><p>总计来说，广义线性模型通过假设一个概率分布函数，得到不同的模型，二支起拿讨论的梯度下降法、牛顿法都是为了求取线性模型中的<strong>线性部分$(\theta ^Tx)$的参数$\theta$的</strong>。</p><p>参考链接：</p><ol><li><a href="https://zh.wikipedia.org/wiki/%E5%BB%A3%E7%BE%A9%E7%B7%9A%E6%80%A7%E6%A8%A1%E5%9E%8B" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/廣義線性模型</a></li><li><a href="http://blog.csdn.net/stdcoutzyx/article/details/9207047" target="_blank" rel="noopener">http://blog.csdn.net/stdcoutzyx/article/details/9207047</a></li></ol>]]></content>
    
    <summary type="html">
    
      广义线性模型（generalized linear model, GLM)是简单最小二乘回归（OLS)的扩展,在广义线性模式中，假设每个变量的观测值Y来自某个指数族分布。
    
    </summary>
    
    
      <category term="Mathematic" scheme="http://yoursite.com/tags/Mathematic/"/>
    
      <category term="统计学" scheme="http://yoursite.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>MCMC与Gibbs采样</title>
    <link href="http://yoursite.com/2017/01/25/MCMC%E4%B8%8EGibbs%E9%87%87%E6%A0%B7/"/>
    <id>http://yoursite.com/2017/01/25/MCMC与Gibbs采样/</id>
    <published>2017-01-24T16:00:00.000Z</published>
    <updated>2018-02-24T06:23:35.800Z</updated>
    
    <content type="html"><![CDATA[<h2>随机模拟</h2><p>  随机模拟(或者统计模拟)方法有一个很酷的别名是蒙特卡罗方法(Monte Carlo Simulation)。这个方法的发展始于20世纪40年代，和原子弹制造的曼哈顿计划密切相关，当时的几个大牛，包括乌拉姆、冯.诺依曼、费米、费曼、Nicholas Metropolis， 在美国洛斯阿拉莫斯国家实验室研究裂变物质的中子连锁反应的时候，开始使用统计模拟的方法,并在最早的计算机上进行编程实现。</p><a id="more"></a><p>&lt;img =src=“<a href="https://cos.name/wp-content/uploads/2013/01/simulation.jpg" target="_blank" rel="noopener">https://cos.name/wp-content/uploads/2013/01/simulation.jpg</a>” width=“50%” height=“50%”&gt;</p><div align="center">simulation随机模拟与计算机</div><p>  现代的统计模拟方法最早由数学家乌拉姆提出，被Metropolis命名为蒙特卡罗方法，蒙特卡罗是著名的赌场，赌博总是和统计密切关联的，所以这个命名风趣而贴切，很快被大家广泛接受。被不过据说费米之前就已经在实验中使用了，但是没有发表。说起蒙特卡罗方法的源头，可以追溯到18世纪，布丰当年用于计算$\pi$的著名的投针实验就是蒙特卡罗模拟实验。统计采样的方法其实数学家们很早就知道，但是在计算机出现以前，随机数生成的成本很高，所以该方法也没有实用价值。随着计算机技术在二十世纪后半叶的迅猛发展，随机模拟技术很快进入实用阶段。对那些用确定算法不可行或不可能解决的问题，蒙特卡罗方法常常为人们带来希望。</p><p><img src="https://cos.name/wp-content/uploads/2013/01/monte-carlo-simulation.jpg" alt="log2"></p><div align="center">monte-carlo-simulation蒙特卡罗方法</div><p>  统计模拟中有一个重要的问题就是给定一个概率分布$p(x)$，我们如何在计算机中生成它的样本。一般而言均匀分布 $Uniform(0,1)$的样本是相对容易生成的。通过线性同余发生器可以生成伪随机数，我们用确定性算法生成[0,1]之间的伪随机数序列后，这些序列的各种统计指标和均匀分布 $Uniform(0,1)$ 的理论计算结果非常接近。这样的伪随机序列就有比较好的统计性质，可以被当成真实的随机数使用。</p><p><img src="https://cos.name/wp-content/uploads/2013/01/sampling.png" alt="">生成一个概率分布的样本</p><p>  而我们常见的概率分布，无论是连续的还是离散的分布，都可以基于$Uniform(0,1)$的样本生成。例如正态分布可以通过著名的 Box-Muller 变换得到。</p><p><strong>Box-Muller 变换</strong>如果随机变量 $U1,U2$独立且$U1,U2\sim Uniform[0,1]$</p><p>$$Z_0 = \sqrt{-2\ln U_1} cos(2\pi U_2) \\ Z_1 = \sqrt{-2\ln U_1} sin(2\pi U_2)$$则 $Z_0,Z_1$独立且服从标准正态分布。</p><p>其它几个著名的连续分布，包括<strong>指数分布、Gamma 分布、t 分布、F 分布、Beta 分布、Dirichlet 分布</strong>等等,也都可以通过类似的数学变换得到；离散的分布通过均匀分布更加容易生成。更多的统计分布如何通过均匀分布的变换生成出来，大家可以参考统计计算的书，其中 Sheldon M. Ross 的《统计模拟》是写得非常通俗易懂的一本。</p><p>不过我们并不是总是这么幸运的，当p(x)的形式很复杂，或者p(x) 是个高维的分布的时候，样本的生成就可能很困难了。 譬如有如下的情况</p><ul><li>$p(x)=\frac{\tilde{p}(x)}{\int\tilde{p}(x)}dx $,而$\tilde{p}(x)$我们是可以计算的，但是底下的积分式无法显式计算。</li><li>$p(x,y)$是一个二维的分布函数，这个函数本身计算很困难，但是条件分布 $p(x|y),p(y|x)$的计算相对简单;如果 $p(x)$是高维的，这种情形就更加明显。</li></ul><p>此时就需要使用一些更加复杂的随机模拟的方法来生成样本。而本节中将要重点介绍的 **MCMC(Markov Chain Monte Carlo) **和 Gibbs Sampling算法就是最常用的一种，这两个方法在现代贝叶斯分析中被广泛使用。要了解这两个算法，我们首先要对马氏链的平稳分布的性质有基本的认识。</p><h2>马氏链及其平稳分布</h2><p>马氏链的数学定义很简单</p><p>$$P(X_{t+1}=x|X_t, X_{t-1}, \cdots) =P(X_{t+1}=x|X_t)$$</p><p>也就是状态转移的概率只依赖于前一个状态。</p><p>我们先来看马氏链的一个具体的例子。社会学家经常把人按其经济状况分成3类：下层(lower-class)、中层(middle-class)、上层(upper-class)，我们用1,2,3 分别代表这三个阶层。社会学家们发现决定一个人的收入阶层的最重要的因素就是其父母的收入阶层。如果一个人的收入属于下层类别，那么他的孩子属于下层收入的概率是 0.65, 属于中层收入的概率是 0.28, 属于上层收入的概率是 0.07。事实上，从父代到子代，收入阶层的变化的转移概率如下</p><p><img src="https://cos.name/wp-content/uploads/2013/01/table-1.jpg" alt="log3"></p><p>使用矩阵的表示方式，转移概率矩阵记为$$P =\begin{bmatrix}0.65 &amp; 0.28 &amp; 0.07 \\0.15 &amp; 0.67 &amp; 0.18 \\0.12 &amp; 0.36 &amp; 0.52 \\\end{bmatrix}$$</p><p>假设当前这一代人处在下层、中层、上层的人的比例是概率分布向量$ \pi_0=[\pi_0(1), \pi_0(2), \pi_0(3)]$那么他们的子女的分布比例将是 $\pi_{1}=\pi_{0}P$, 他们的孙子代的分布比例将是 $\pi_{2} = \pi_{1}P=\pi_{0}P^2$第n代子孙的收入分布比例将是$\pi_{n} = \pi_{n-1}P = \pi_{0}P^n$假设初始概率分布为$\pi_{0}=[0.21,0.68,0.11]$,则我们可以计算前n代人的分布状况如下</p><p><img src="https://cos.name/wp-content/uploads/2013/01/table-2.jpg" alt="log2"></p><p>我们发现从第7代人开始，这个分布就稳定不变了，这个是偶然的吗？我们换一个初始概率分布$\pi _0=[0.75,0.15,0.1]$试试看，继续计算前n代人的分布状况如下<img src="https://cos.name/wp-content/uploads/2013/01/table-3.jpg" alt="log3">我们发现，到第9代人的时候, 分布又收敛了。最为奇特的是，两次给定不同的初始概率分布，最终都收敛到概率分布 $\pi=[0.286,0.489,0.225]$，也就是说收敛的行为和初始概率分布 $\pi_0$ 无关。这说明这个收敛行为主要是由概率转移矩阵P决定的。我们计算一下 $P_n$$P^{20} = P^{21} = \cdots = P^{100} = \cdots = \begin{bmatrix} 0.286 &amp; 0.489 &amp; 0.225 \\ 0.286 &amp; 0.489 &amp; 0.225 \\ 0.286 &amp; 0.489 &amp; 0.225 \\ \end{bmatrix}$我们发现，当 n 足够大的时候，这个$P_n$矩阵的每一行都是稳定地收敛到$\pi=[0.286,0.489,0.225]$这个概率分布。自然的，这个收敛现象并非是我们这个马氏链独有的，而是绝大多数马氏链的共同行为，关于马氏链的收敛我们有如下漂亮的定理：</p><h3>马氏链定理：</h3><p>如果一个非周期马氏链具有转移概率矩阵PP,且它的任何两个状态是连通的，那么$\displaystyle \lim_{n\rightarrow\infty}P_{ij}^n$存在且与i无关，记$\displaystyle \lim_{n\rightarrow\infty}P_{ij}^n = \pi(j)$, 我们有</p><ol><li><p>$\displaystyle \lim_{n \rightarrow \infty} P^n =\begin{bmatrix} \pi(1) &amp; \pi(2) &amp; \cdots &amp; \pi(j) &amp; \cdots \\ \pi(1) &amp; \pi(2) &amp; \cdots &amp; \pi(j) &amp; \cdots \\ \cdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots \\ \pi(1) &amp; \pi(2) &amp; \cdots &amp; \pi(j) &amp; \cdots \\ \cdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots \\ \end{bmatrix}$</p></li><li><p>$\displaystyle \pi(j) = \sum_{i=0}^{\infty}\pi(i)P_{ij}$</p></li><li><p>$\pi$是方程$\pi P=\pi $的唯一非负解其中,$$\pi = [\pi(1), \pi(2), \cdots, \pi(j),\cdots ], \\ \sum_{i=0}^{\infty} \pi_i = 1$$</p></li></ol><p>$\pi$称为马氏链的平稳分布。</p><p>这个马氏链的收敛定理非常重要，所有的 MCMC(Markov Chain Monte Carlo) 方法都是以这个定理作为理论基础的。 定理的证明相对复杂，一般的随机过程课本中也不给证明，所以我们就不用纠结它的证明了，直接用这个定理的结论就好了。我们对这个定理的内容做一些解释说明：</p><ol><li>该定理中马氏链的状态不要求有限，可以是有无穷多个的；</li><li>定理中的“非周期“这个概念我们不打算解释了，因为我们遇到的绝大多数马氏链都是非周期的；</li><li>两个状态i,j是连通并非指i可以直接一步转移到$j(P_{i,j}&gt;0)$,而是指 i 可以通过有限的n步转移到达$j(P_{ij&gt;}^n&gt;0)$。马氏链的任何两个状态是连通的含义是指存在一个n, 使得矩阵$P_n$ 中的任何一个元素的数值都大于零。</li><li>我们用 $X_i$ 表示在马氏链上跳转第i步后所处的状态，如果 $\displaystyle \lim_{n\rightarrow\infty}P_{ij}^n = \pi(j)$存在，很容易证明以上定理的第二个结论。由于</li></ol><p>$$P(X_{n+1}=j)=\\ \sum_{i=0}^\infty P(X_n=i) P(X_{n+1}=j|X_n=i) \  = \sum_{i=0}^\infty P(X_n=i) P_{ij} $$</p><p>上式两边取极限就得到$\displaystyle \pi(j) = \sum_{i=0}^{\infty}\pi(i)P_{ij}$</p><p>从初始概率分布$\pi_0$出发，我们在马氏链上做状态转移，记$x_i$的概率分布为$\pi_i$, 则有</p><p>$$X_0 \sim \pi_0(x) \\ X_i \sim \pi_i(x), \\ \pi_i(x) = \pi_{i-1}(x)P = \pi_0(x)P^n$$</p><p>由马氏链收敛的定理, 概率分布$\pi_i(x)$将收敛到平稳分布$\pi(x)$。假设到第n步的时候马氏链收敛，则有</p><p>$$ X_0 \sim \pi_0(x) \\ X_1 \sim \pi_1(x) \\ \cdots \\ X_n \sim \pi_n(x)=\pi(x) \\ X_{n+1} \sim \pi(x) \\ X_{n+2} \sim \pi(x) \\ \cdots $$</p><p>所以 $X_n,X_{n+1},X_{n+2},\cdots \sim \pi(x)$都是同分布的随机变量，当然他们并不独立。如果我们从一个具体的初始状态 $x_0$开始,沿着马氏链按照概率转移矩阵做跳转，那么我们得到一个转移序列 $x_0,x_1,x_2,\cdots,x_n,x_{n+1},\cdots,$ 由于马氏链的收敛行为， $x_n,x_{n+1},\cdots$都将是平稳分布$\pi(x)$的样本。</p><h2>Markov Chain Monte Carlo</h2><p>  对于给定的概率分布p(x),我们希望能有便捷的方式生成它对应的样本。由于马氏链能收敛到平稳分布， 于是一个很的漂亮想法是：如果我们能构造一个转移矩阵为$P$的马氏链，使得该马氏链的平稳分布恰好是$p(x)$, 那么我们从任何一个初始状态$x_0$出发沿着马氏链转移, 得到一个转移序列 $x_0,x_1,x_2,\cdots x_n,x_{n+1},\cdots,$， 如果马氏链在第n步已经收敛了，于是我们就得到了 $\pi(x)$的样本$x_n,x_{n+1},\cdots,x_n,x_{n+1},\cdots$。</p><p>  这个绝妙的想法在1953年被 Metropolis想到了，为了研究粒子系统的平稳性质， Metropolis 考虑了物理学中常见的波尔兹曼分布的采样问题，首次提出了基于马氏链的蒙特卡罗方法，即Metropolis算法，并在最早的计算机上编程实现。Metropolis 算法是首个普适的采样方法，并启发了一系列 MCMC方法，所以人们把它视为随机模拟技术腾飞的起点。 Metropolis的这篇论文被收录在《统计学中的重大突破》中， Metropolis算法也被遴选为二十世纪的十个最重要的算法之一。</p><p>  我们接下来介绍的MCMC 算法是 Metropolis 算法的一个改进变种，即常用的 Metropolis-Hastings 算法。由上一节的例子和定理我们看到了，马氏链的收敛性质主要由转移矩阵$P$决定, 所以基于马氏链做采样的关键问题是如何构造转移矩阵$P$,使得平稳分布恰好是我们要的分布$p(x)$。如何能做到这一点呢？我们主要使用如下的定理。</p><h3>定理：细致平稳条件</h3><p>如果非周期马氏链的转移矩阵$P$和分布$\pi(x)$满足$$ \pi(i)P_{ij} = \pi(j)P_{ji} \qquad for all\quad i,j$$</p><p>则 $\pi(x)$ 是马氏链的平稳分布，上式被称为细致平稳条件(detailed balance condition)。</p><p>其实这个定理是显而易见的，因为细致平稳条件的物理含义就是<strong>对于任何两个状态i,j从 i 转移出去到j 而丢失的概率质量，恰好会被从 j 转移回i 的概率质量补充回来，所以状态i上的概率质量$\pi(i)$是稳定的，从而$\pi(x)$是马氏链的平稳分布</strong>。数学上的证明也很简单，由细致平稳条件可得</p><p>$$\sum_{i=1}^\infty \pi(i)P_{ij} =\sum_{i=1}^\infty \pi(j)P_{ji} = \pi(j) \sum_{i=1}^\infty P_{ji} = \pi(j) \\ \Rightarrow \pi P = \pi$$</p><p>由于$\pi$是方程 $\pi P=\pi $的解，所以$\pi $是平稳分布。</p><p>假设我们已经有一个转移矩阵为Q马氏链$q(i,j)$表示从状态 i转移到状态j的概率，也可以写为 $q(j|i)$ 显然，通常情况下</p><p>$$p(i) q(i,j) \neq p(j) q(j,i)$$</p><p>也就是细致平稳条件不成立，所以$p(x)$不太可能是这个马氏链的平稳分布。我们可否对马氏链做一个改造，使得细致平稳条件成立呢？譬如，我们引入一个 $\alpha(i,j)$, 我们希望</p><p>$$p(i) q(i,j)\alpha(i,j) = p(j) q(j,i)\alpha(j,i)  \quad (*) $$</p><p>$$\alpha(i,j)= p(j) q(j,i),\quad \alpha(j,i) = p(i) q(i,j)$$</p><p>于是(*)式就成立了。所以有</p><p>$$p(i)\underbrace{q(i,j)\alpha(i,j)}<em>{Q^{’}(i,j)} = p(j)\underbrace{q(j,i)\alpha(j,i)}</em>{Q^{’}(j,i)}$$</p><p>于是我们把原来具有转移矩阵$Q$的一个很普通的马氏链，改造为了具有转移矩阵$Q’$的马氏链，而$Q’$恰好满足细致平稳条件，由此马氏链$Q’$的平稳分布就是$p(x)$！</p><p>在改造 Q 的过程中引入的 $\alpha(i,j)$称为接受率，物理意义可以理解为在原来的马氏链上**，从状态 i 以$q(i,j)$ 的概率转跳转到状态j 的时候，我们以$\alpha (i,j)$的概率接受这个转移，于是得到新的马氏链Q′的转移概率为$q(i,j)\alpha(i,j)$。**</p><p><img src="https://cos.name/wp-content/uploads/2013/01/mcmc-transition.jpg" alt=""></p><p>假设我们已经有一个转移矩阵$Q$(对应元素为$q(i,j)$, 把以上的过程整理一下，我们就得到了如下的用于采样概率分布$p(x)$的算法。</p><p><img src="https://cos.name/wp-content/uploads/2013/01/mcmc-algo-1.jpg" alt="">上述过程中 $p(x),q(x|y)$ 说的都是离散的情形，事实上即便这两个分布是连续的，以上算法仍然是有效，于是就得到更一般的连续概率分布 $p(x)$的采样算法，而 $q(x|y)$ 就是任意一个连续二元概率分布对应的条件分布。</p><p>  以上的 MCMC 采样算法已经能很漂亮的工作了，不过它有一个小的问题：马氏链Q在转移的过程中的接受率 $\alpha(i,j)$ 可能偏小，这样采样过程中马氏链容易原地踏步，拒绝大量的跳转，这使得马氏链遍历所有的状态空间要花费太长的时间，收敛到平稳分布$p(x)$的速度太慢。有没有办法提升一些接受率呢?</p><p>假设 $\alpha(i,j)=0.1,\alpha(j,i)=0.2$, 此时满足细致平稳条件，于是</p><p>$$p(i)q(i,j)\times 0.1 = p(j)q(j,i) \times 0.2$$</p><p>上式两边扩大5倍，我们改写为$$p(i)q(i,j) \times 0.5 = p(j)q(j,i) \times 1$$</p><p>看，我们提高了接受率，而细致平稳条件并没有打破！这启发我们可以把细致平稳条件(**) 式中的$\alpha(i,j),\alpha(j,i)$同比例放大，使得两数中最大的一个放大到1，这样我们就提高了采样中的跳转接受率。所以我们可以取</p><p>$$\alpha(i,j) = \min{\frac{p(j)q(j,i)}{p(i)q(i,j)},1}$$</p><p>于是，经过对上述MCMC 采样算法中接受率的微小改造，我们就得到了如下教科书中最常见的 Metropolis-Hastings 算法。<img src="https://cos.name/wp-content/uploads/2013/01/mcmc-algo-2.jpg" alt="log03"></p><p>  对于分布 $p(x)$,我们构造转移矩阵 Q′ 使其满足细致平稳条件</p><p>$$p(x) Q’(x\rightarrow y) = p(y) Q’(y\rightarrow x)$$</p><p>此处$x$并不要求是一维的，对于高维空间的$p(x)$，如果满足细致平稳条件</p><p>$$p(\mathbf{x}) Q’(\mathbf{x}\rightarrow \mathbf{y}) = p(\mathbf{y}) Q’(\mathbf{y}\rightarrow \mathbf{x})$$那么以上的 Metropolis-Hastings 算法一样有效。</p><h2>Gibbs Sampling</h2><p>  对于高维的情形，由于接受率$\alpha$的存在(通常$\alpha&lt;1$), 以上 Metropolis-Hastings 算法的效率不够高。能否找到一个转移矩阵$Q$使得接受率$\alpha=1$呢？我们先看看二维的情形，假设有一个概率分布 $p(x,y)$, 考察x坐标相同的两个点$A(x_1,y_1),B(x_1,y_2)$，我们发现$p(x_1,y_1)p(y_2|x_1) = p(x_1)p(y_1|x_1)p(y_2|x_1) \ p(x_1,y_2)p(y_1|x_1) = p(x_1)p(y_2|x_1)p(y_1|x_1)$</p><p>所以得到$ p(x_1,y_1)p(y_2|x_1) = p(x_1,y_2)p(y_1|x_1)  \quad (***) $即$p(A)p(y_2|x_1) = p(B)p(y_1|x_1)$</p><p><img src="https://cos.name/wp-content/uploads/2013/01/gibbs-transition.png" alt="">平面上马氏链转移矩阵的构造</p><p>$Q(A\rightarrow B) = p(y_B|x_1) \quad \text{if} \quad x_A=x_B=x_1 \\ Q(A\rightarrow C) = p(x_C|y_1) \quad \text{if} \quad y_A=y_C=y_1 \\ Q(A\rightarrow D) = 0 \quad ,\text{others}$</p><p>有了如上的转移矩阵$Q$, 我们很容易验证对平面上任意两点$X,Y$ 满足细致平稳条件</p><p>$$p(X)Q(X\rightarrow Y) = p(Y) Q(Y\rightarrow X)$$</p><p>于是这个二维空间上的马氏链将收敛到平稳分布 p(x,y)p(x,y)。而这个算法就称为 Gibbs Sampling 算法,是 Stuart Geman 和Donald Geman 这两兄弟于1984年提出来的，之所以叫做Gibbs Sampling 是因为他们研究了Gibbs random field, 这个算法在现代贝叶斯分析中占据重要位置。<img src="https://cos.name/wp-content/uploads/2013/01/gibbs-algo-1.jpg" alt=""><img src="https://cos.name/wp-content/uploads/2013/01/two-stage-gibbs.png" alt=""></p><h3>Gibbs Sampling 算法中的马氏链转移</h3><p>  以上采样过程中，如图所示，马氏链的转移只是轮换的沿着坐标轴 xx轴和yy轴做转移，于是得到样本$(x_0,y_0), (x_0,y_1), (x_1,y_1), (x_1,y_2),(x_2,y_2), \cdots$马氏链收敛后，最终得到的样本就是 p(x,y)的样本，而收敛之前的阶段称为 burn-in period。额外说明一下，我们看到教科书上的 Gibbs Sampling 算法大都是坐标轴轮换采样的，但是这其实是不强制要求的。最一般的情形可以是，在t时刻，可以在x轴和y轴之间随机的选一个坐标轴，然后按条件概率做转移，马氏链也是一样收敛的。轮换两个坐标轴只是一种方便的形式。</p><p>  以上的过程我们很容易推广到高维的情形，对于(***) 式，如果$x_1$ 变为多维情形$x_1$，可以看出推导过程不变，所以细致平稳条件同样是成立的</p><p>$ p(\mathbf{x_1},y_1)p(y_2|\mathbf{x_1}) = p(\mathbf{x_1},y_2)p(y_1|\mathbf{x_1}) $此时转移矩阵 Q 由条件分布 $p(y|x_1)$定义。上式只是说明了一根坐标轴的情形，和二维情形类似，很容易验证对所有坐标轴都有类似的结论。所以n维空间中对于概率分布$ p(x1,x2,\cdots ,xn)$ 可以如下定义转移矩阵</p><ol><li>如果当前状态为$(x_1,x_2,\cdots ,x_n)$，马氏链转移的过程中，只能沿着坐标轴做转移。沿着 $x_i$这根坐标轴做转移的时候，转移概率由条件概率 $p(x_i|x_1, \cdots, x_{i-1}, x_{i+1}, \cdots, x_n)$定义；</li><li>其它无法沿着单根坐标轴进行的跳转，转移概率都设置为 0。</li></ol><p>于是我们可以把Gibbs Smapling 算法从采样二维的$p(x,y)$推广到采样n 维的 $p(x_1,x_2,\cdots,x_n)$</p><p><img src="https://cos.name/wp-content/uploads/2013/01/gibbs-algo-2.jpg" alt=""></p><p>以上算法收敛后，得到的就是概率分布$p(x_1,x_2,\cdots ,x_n)$的样本，当然这些样本并不独立，但是我们此处要求的是采样得到的样本符合给定的概率分布，并不要求独立。同样的，在以上算法中，坐标轴轮换采样不是必须的，可以在坐标轴轮换中引入随机性，这时候转移矩阵$Q.$ 中任何两个点的转移概率中就会包含坐标轴选择的概率，而在通常的 Gibbs Sampling 算法中，坐标轴轮换是一个确定性的过程，也就是在给定时刻t，在一根固定的坐标轴上转移的概率是1。</p><blockquote><p>转载自<a href="https://cos.name/2013/01/lda-math-mcmc-and-gibbs-sampling/" target="_blank" rel="noopener">统计之都</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2&gt;随机模拟&lt;/h2&gt;
&lt;p&gt;  随机模拟(或者统计模拟)方法有一个很酷的别名是蒙特卡罗方法(Monte Carlo Simulation)。这个方法的发展始于20世纪40年代，和原子弹制造的曼哈顿计划密切相关，当时的几个大牛，包括乌拉姆、冯.诺依曼、费米、费曼、Nicholas Metropolis， 在美国洛斯阿拉莫斯国家实验室研究裂变物质的中子连锁反应的时候，开始使用统计模拟的方法,并在最早的计算机上进行编程实现。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Mathematic" scheme="http://yoursite.com/tags/Mathematic/"/>
    
      <category term="统计学" scheme="http://yoursite.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>指数分布族</title>
    <link href="http://yoursite.com/2017/01/23/%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83%E6%97%8F/"/>
    <id>http://yoursite.com/2017/01/23/指数分布族/</id>
    <published>2017-01-22T16:00:00.000Z</published>
    <updated>2018-02-23T11:01:25.196Z</updated>
    
    <content type="html"><![CDATA[<p>  指数分布族是指可以表示为指数分布的概率分布。指数分布形式如下：$$P(y;\eta)=b(y)exp(\eta^{T}T(y)-\alpha(\eta))$$其中，$\eta$成为分布的自然参数；T(y)是充分统计量，通常$T(y)=y$。当$a、b、T$参数都固定的时候，就定义了一个以$\eta$为参数的指数函数族。</p><a id="more"></a><p>实际上，大多数概率分布都可以表示成上面公式给出的形式：</p><ol><li>伯努利分布：对0、1问题进行建模</li><li>多项式分布：对K个离散结果的事件建模</li><li>泊松分布：对计数过程进行建模</li><li>伽马分布与指数分布：对间隔的正数进行建模</li><li>Beta分布：对小数进行建模</li><li>Dirichlet分布：对小数进行建模</li><li>Wishart分布：对协方差进行建模</li><li>高斯分布</li></ol><h2>示例</h2><p>我们将高斯分布与伯努利分布表示成指数分布族的形式。</p><h3>伯努利分布</h3><p>  伯努利分布是对0、1问题进行建模，特可以表示成如下形式：</p><p>$$P(y;\varphi)=\varphi^y(1-\varphi)^{1-y} \quad y\in{0,1}$$</p><p><img src="http://img.blog.csdn.net/20170226102110215?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="log01"></p><p>将伯努利分布表示成如下形式，对比指数族分布公示</p><p><img src="http://img.blog.csdn.net/20170226102145230?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="log02"></p><p>其中可以看到，$\eta$的形式为logisic函数，这是因为logistic模型对问题的先验概率估计是伯努利分布的缘故。</p><h3>高斯分布</h3><p>  高斯分布可以推导出线性模型，由线性模型的假设函数可知，高斯分布的方差与假设函数无关，因而为简便计算，我们将方差设为1，即使不这样做，最后的结果也是作为一个系数而已，高斯分布转换为指数分布形式的推导过程如下：</p><p><img src="http://img.blog.csdn.net/20170226102549656?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="log03"></p><p>我们最终一样可以把高斯分布以指数分布族函数的形式表示。</p><h2>后记</h2><ol><li>这里说明指数分布族的目的，是为了说明关于线性模型(Generalized Linear Model).</li><li>凡是符合指数分布族的随机变量，都可以用广义线性模型(GLM)进行分析。</li></ol><h3>备注</h3><p>指数分布族的<strong>无记忆性</strong>，教科书上所说的无记忆性（Memoryless Property，又称遗失记忆性）。这表示如果一个随机变量呈指数分布，它的条件概率遵循：$$P(T&gt;s+t;T&gt;t)=P(T&gt;s), \ for\quad all,\quad s.t&gt;0 $$有兴趣的同学可以深入理解一下。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;  指数分布族是指可以表示为指数分布的概率分布。指数分布形式如下：
$$P(y;\eta)=b(y)exp(\eta^{T}T(y)-\alpha(\eta))$$
其中，$\eta$成为分布的自然参数；T(y)是充分统计量，通常$T(y)=y$。当$a、b、T$参数都固定的时候，就定义了一个以$\eta$为参数的指数函数族。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Mathematic" scheme="http://yoursite.com/tags/Mathematic/"/>
    
      <category term="统计学" scheme="http://yoursite.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>矩阵分解相关知识回顾</title>
    <link href="http://yoursite.com/2017/01/21/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE/"/>
    <id>http://yoursite.com/2017/01/21/矩阵分解相关知识回顾/</id>
    <published>2017-01-20T16:00:00.000Z</published>
    <updated>2018-02-24T06:28:40.979Z</updated>
    
    <content type="html"><![CDATA[<h2>特征值与特征向量</h2><p>设A是数域F上的n阶矩阵，如果存在数域F中的一个数$\lambda$与数域上F的非零向量$\overrightarrow{\alpha}$，使得：$A\overrightarrow{\alpha}=\lambda \overrightarrow{\alpha}$则称$\lambda$为A的一个特征值(根)(eigenvalue)，称$\overrightarrow{\alpha}$为A的属于特征值$\lambda$的特征向量(eigenvector)。</p><p>显然从上式可以看出，$A\overrightarrow{\alpha} \overrightarrow{\alpha}$平行。</p><p>将上式做一下变换：$A\overrightarrow{\alpha}=\lambda \overrightarrow{\alpha}$$A\overrightarrow{\alpha}-\lambda \overrightarrow{\alpha}=0$$A\overrightarrow{\alpha}-\lambda E\overrightarrow{\alpha}=0$$(A−\lambda E)\overrightarrow{\alpha}=0$$(\lambda E-A)\overrightarrow{\alpha}=0$</p><p>称：$\lambda E−A$为A的特征矩阵行列式$f(\lambda )=|\lambda E−A|$为A的特征多项式$|\lambda E−A|=0$为A的特征方程$(\lambda E−A)\overrightarrow{x}=\overrightarrow{0}$是A关于该λ的齐次线性方程组</p><h2>矩阵对角化</h2><p>设n阶方阵A存在n个线性无关的特征向量$\overrightarrow{ x_{i} }$，将这n个特征向量$\overrightarrow {x_{i} }$组成方阵S(也称为特征向量矩阵），则有：<img src="http://img.blog.csdn.net/20170225194650882?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">这个式子称为$A$的$SΛS^{−1}$分解，或特征分解(Eigendecomposition)，或A的对角化。</p><p>根据这个式子可以知道：当方阵$A$可以被分解为某个矩阵$S$乘以某个对角矩阵$\Lambda$再乘以矩阵$S^{−1}$时，就是一次特征分解。</p><p>可以对角化的前提是$A$有$n$个线性无关的特征向量。$A$有$n$个线性无关的特征向量的前提是，所有的$\lambda$都不重复（没有重根）。</p><h2>LU分解</h2><p>设$A$是一个方块矩阵。A的$LU$分解是将它分解成如下形式：$A=LU$其中$L$和$U$分别是下三角矩阵和上三角矩阵。</p><p>例如对于一个 $3*3$的矩阵，就有</p><p><img src="http://img.blog.csdn.net/20170225191810280?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="null"></p><p>一个$LDU$分解是一个如下形式的分解：$A=LDU$其中$D$是对角矩阵，$L$和$U$是单位三角矩阵（对角线上全是1的三角矩阵）。</p><p>一个$LUP$分解是一个如下形式的分解：$A=LUP$其中$L$和$U$仍是三角矩阵，$P$是一个置换矩阵。</p><p>一个充分消元的$LU$分解为如下形式：$PAQ=LU$</p><h3>存在性</h3><p>一个可逆矩阵可以进行$LU$分解当且仅当它的所有子式都非零。如果要求其中的$L$矩阵（或$U$矩阵）为单位三角矩阵，那么分解是唯一的。同理可知，矩阵的$LDU$可分解条件也相同，并且总是唯一的。</p><h2>奇异值分解</h2><p>假设M是一个m×n阶矩阵，其中的元素全部属于域K，也就是实数域或复数域。如此则存在一个分解使得$M=U\sum V^{*}$其中U是m×m阶酉矩阵；$\sum$是m×n阶非负实数对角矩阵；而$V^{*}$，即V的共轭转置，是n×n阶酉矩阵。这样的分解就称作$M$的奇异值分解。</p><h3>几何解释</h3><p>首先，我们来看一个只有两行两列的简单矩阵。第一个例子是对角矩阵<img src="http://img.blog.csdn.net/20170225201907278?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="log00">从几何的角度，矩阵可以描述为一个变换：用矩阵乘法将平面上的点（x, y）变换成另外一个点（3x, y）：</p><p><img src="http://img.blog.csdn.net/20170225201947559?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="log01">这种变换的效果如下：平面在水平方向被拉伸了3倍，在竖直方向无变化。<img src="http://img.blog.csdn.net/20170225202039887?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="log02"></p><p>$2*2$矩阵奇异值分解的几何实质是：对于任意$2*2$矩阵，总能找到某个正交网格到另一个正交网格的转换与矩阵变换相对应。</p><p>用向量解释这个现象：选择适当的正交的单位向量$v_1$和$v_2$，向量$Mv_1$和$Mv_2$也是正交的。<img src="http://img.blog.csdn.net/20170225201208758?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdG91cmlzdG1hbjU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="log05"></p><p>奇异值分解的魅力在于任何矩阵都可以找到奇异值。</p><p>参考链接：</p><ol><li><a href="http://www.ams.org/samplings/feature-column/fcarc-svd" target="_blank" rel="noopener">http://www.ams.org/samplings/feature-column/fcarc-svd</a></li><li><a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html" target="_blank" rel="noopener">https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      大学线性代数课程中我们学习了很多关于矩阵分解的方法，这些在概率统计、统计机器学习等方面都有很多应用。
    
    </summary>
    
    
      <category term="Mathematic" scheme="http://yoursite.com/tags/Mathematic/"/>
    
      <category term="统计学" scheme="http://yoursite.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>优化算法篇之梯度法</title>
    <link href="http://yoursite.com/2017/01/15/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%AF%87%E4%B9%8B%E6%A2%AF%E5%BA%A6%E6%B3%95/"/>
    <id>http://yoursite.com/2017/01/15/优化算法篇之梯度法/</id>
    <published>2017-01-14T17:41:05.000Z</published>
    <updated>2018-02-23T16:46:52.777Z</updated>
    
    <content type="html"><![CDATA[<p>  我们在接触到具体的机器学习算法前，其实很有必要对优化问题进行一些了解。随着学习的深入，越来越发现最优化方法的重要性，学习和工作中遇到的大多问题都可以建模成一种最优化模型进行求解，比如我们现在学习的机器学习算法，大部分的机器学习算法的本质都是建立优化模型，通过最优化方法对目标函数（或损失函数）进行优化，从而训练出最好的模型。</p><a id="more"></a><p>  最常见的最优化方法有梯度下降法、牛顿法和拟牛顿法、共轭梯度方向法等。在大学课程中，数值分析是计算机或数学相关专业一门比较重要的一门课程，笔者也在大学时自学过相关课程，其介绍的诸多对理论的计算机实现方法，对现在的学习依然发挥着很大的作用。</p><p>当然优化算法只是数值分析课程中涉及一部分内容，这一节主要介绍和回顾梯度下降法</p><h2>梯度下降法</h2><p>  梯度下降法是最为常见的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。在一般情况下，其解不能保证是全局最优解，梯度下降法的速度也未必是最快的。</p><p>  假设$f(x)$是$R^n$上具有一阶连续偏导数的函数，需要求解的无约束最优化问题是：</p><p>$${min}_{x\in R^n}：\quad f(x)$$</p><p>  梯度下降法是一种迭代算法，选取适当的初值$x_0$,反复迭代，更新$x_{i}$的值，进行目标函数的极小化，直至收敛。<img src="http://img.my.csdn.net/uploads/201302/13/1360748597_8621.jpg" alt="梯度方向"></p><p>  由于我们都知道梯度方向$∇f(x)$是函数增长最快的方向，那么自然而然的想到负梯度方向就是函数值下降最快的方向了。因此，我们以负梯度方向作为极小化的下降方向，在迭代的每一步，以负梯度方向来更新$x$的值，从而达到减小函数值目的，这种方法就是梯度下降法。</p><p>  由于$f(x)$具有一阶连续偏导数，若第k次迭代值为$x_k$，则可将$f(x)$在$x_k$处进行一阶泰勒展开：$$f(x)=f(x^{(k)})+g_k^T(x−x^{(k)})$$这里，(方便推广，使用矩阵形式）$$g_k=g(x^{(k)})=∇f(x^{(k)})g_k=g(x^{(k)})=∇f(x^{(k)})$$为$f(x)$在$x_K$的梯度。第$K+1$次迭代值$x_{k+1}$：即$$x_{k+1}=x_{k}+λ_k*p_k$$其中，$p_k$是搜索方向，梯度法中$p_k=-∇f(x)$，取负梯度方向$p_k=−∇f(x^{(k)}),{\lambda_k}$是步长，有时候我们也叫学习率，这个值可以由一维搜索确定，目的在于得到最合适的步长，即${\lambda _k}$使得</p><p>$$Min:\quad \varphi(\lambda)=f(x^{(k)}+λ_kp^{(k)})$$</p><h3>算法过程：</h3><p>1）确定当前位置的损失函数的梯度，对于$(\theta_i)$,其梯度表达式如下：</p><p>$$(\frac{\partial}{\partial\theta_i}f(\theta_0, \theta_1…, \theta_n))$$</p><p>2）用步长乘以损失函数的梯度，得到当前位置下降的距离，即$(\alpha\frac{\partial}{\partial\theta_i}f(\theta_0, \theta_1…, \theta_n))$对应于前面登山例子中的某一步。</p><p>3）确定是否所有的$(\theta_i)$,梯度下降的距离都小于$\varepsilon$，如果小于$\varepsilon$则算法终止，当前所有的$\theta_i,(i=0,1,…n)$即为最终结果。否则进入步骤4.</p><p>4）更新所有的$\theta$，对于$\theta_i$，其更新表达式如下。更新完毕后继续转入步骤1.</p><p>$\theta_i = \theta_i – \alpha\frac{\partial}{\partial\theta_i}f(\theta_0, \theta_1…, \theta_n)$</p><h3>举例</h3><p>下面用线性回归的例子来具体描述梯度下降。假设我们的样本是$(x_1^{(0)}, x_2^{(0)}, …x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, …x_n^{(1)},y_1), … (x_1^{(m)}, x_2^{(m)}, …x_n^{(m)}, y_n)$,损失函数如前面先决条件所述：</p><p>$$f(\theta_0, \theta_1…, \theta_n) = \sum\limits_{i=0}^{m}(h_\theta(x_0, x_1, …x_n) – y_i)^2$$。</p><p>则在算法过程步骤1中对于$\theta_i$ 的偏导数计算如下：</p><p>$$\frac{\partial}{\partial\theta_i}f(\theta_0, \theta_1…, \theta_n)= \frac{1}{m}\sum\limits_{j=0}^{m}(h_\theta(x_0^{j}, x_1^{j}, …x_n^{j}) – y_j)x_i^{j}$$</p><p>由于样本中没有$x_0$上式中令所有的$x_0^{j}$为1.</p><p>步骤4中$\theta_i$的更新表达式如下：</p><p>$$\theta_i = \theta_i – \alpha\frac{1}{m}\sum\limits_{j=0}^{m}(h_\theta(x_0^{j}, x_1^{j}, …x_n^{j}) – y_j)x_i^{j}$$</p><p>从这个例子可以看出当前点的梯度方向是由所有的样本决定的，加$\frac{1}{m}$ 是为了好理解。由于步长也为常数，他们的乘机也为常数，所以这里$\alpha\frac{1}{m}$可以用一个常数表示。</p><p>梯度下降法的搜索迭代示意图如下图所示:<img src="http://images2015.cnblogs.com/blog/743682/201511/743682-20151108163643227-650396065.png" alt="梯度下降法的搜索迭代示意图如下图所示"></p><p>参考书籍：</p><blockquote><p>An introduction to optimization-最优化导论[J]. Edwin K.P.Chong.</p></blockquote>]]></content>
    
    <summary type="html">
    
      随着学习的深入，越来越发现最优化方法的重要性，学习和工作中遇到的大多问题都可以建模成一种最优化模型进行求解，比如我们现在学习的机器学习算法，大部分的机器学习算法的本质都是建立优化模型，通过最优化方法对目标函数（或损失函数）进行优化，从而训练出最好的模型。
    
    </summary>
    
    
      <category term="Mathematic" scheme="http://yoursite.com/tags/Mathematic/"/>
    
      <category term="优化算法" scheme="http://yoursite.com/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>优化算法篇之牛顿法</title>
    <link href="http://yoursite.com/2017/01/14/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%AF%87%E4%B9%8B%E7%89%9B%E9%A1%BF%E6%B3%95/"/>
    <id>http://yoursite.com/2017/01/14/优化算法篇之牛顿法/</id>
    <published>2017-01-13T16:00:00.000Z</published>
    <updated>2018-02-23T11:06:47.762Z</updated>
    
    <content type="html"><![CDATA[<p>  我们在接触具体的机器学习算法前，其实很有必要对优化问题进行一些介绍。随着学习的深入，笔者越来越发现最优化方法的重要性，学习和工作中遇到的大多问题都可以建模成一种最优化模型进行求解，比如我们现在学习的机器学习算法，大部分的机器学习算法的本质都是建立优化模型，通过最优化方法对目标函数（或损失函数）进行优化，从而训练出最好的模型。</p><a id="more"></a><p>  最常见的最优化方法有梯度下降法、牛顿法和拟牛顿法、共轭梯度方向法等。在大学课程中，数值分析是计算机或数学相关专业一门比较重要的一门课程，笔者也在大学时自学过相关课程，其介绍的诸多对理论的计算机实现方法，对现在的学习依然发挥着很大的作用。当然优化算法只是数值分析课程中涉及一部分内容，这一节主要介绍和回顾牛顿法。</p><h2>牛顿法</h2><p>  上节介绍的梯度下降法（最速下降法）只用到了目标函数的一阶导数，牛顿法是一种二阶优化算法，相对于梯度下降算法收敛速度更快。首先，选择一个接近函数$f(x)$零点的$x_0$，计算相应的$f(x_0)$ 和切线斜率$f ’ (x_0)$。然后我们计算穿过点$(x_0,  f(x_0))$ 并且斜率为$f ‘(x_0)$的直线和 $X$轴的交点的$x$坐标，也就是求如下方程的解：$f(x_0)+f’(x_0)*(x-x_0)=0$我们将新求得的点的$x$坐标命名为$x_1$，通常$x_1$会比$x_0$更接近方程$f(x) = 0$的解。因此我们现在可以利用$x_1$开始下一轮迭代。迭代公式可化简为如下所示：</p><p>$$x_{n+1}=x_n-\frac{f(x_n)}{f’(x_n)}$$  牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是&quot;切线法&quot;。牛顿法的搜索路径（二维情况）如下图所示：</p><p><img src="https://upload.wikimedia.org/wikipedia/commons/e/e0/NewtonIteration_Ani.gif?_=4751804" alt="newton"></p><h4>缺点</h4><p>牛顿法也有很大的缺点，就是每次计算都需要计算Hessian矩阵的逆，因此计算量较大。</p><h3>拟牛顿法</h3><p>  拟牛顿法在一定程度上解决了牛顿法计算量大的问题。其本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它<strong>使用正定矩阵来近似Hessian矩阵的逆</strong>，从而简化了运算的复杂度。（在多变量的情况下，如果目标矩阵的Hessain矩阵非正定，牛顿法确定的搜索方向并不一定是目标函数下降的方向）<br>  拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。</p><h4>拟牛顿法的Matlab实现：</h4><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">#函数名：quasi_Newton(f,x0,error), </span><br><span class="line">#参数：f:待求梯度函数   x0:初始点   error：允许误差 </span><br><span class="line">#主程序： </span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">A</span>=<span class="title">quasi_Newton</span><span class="params">(f,x0,error)</span> </span></span><br><span class="line">     [a,b]=<span class="built_in">size</span>(x0); </span><br><span class="line">     G0=<span class="built_in">eye</span>(b); </span><br><span class="line">     initial_gradient=gradient_my(f,x0,b); </span><br><span class="line">     norm0=<span class="number">0</span>; </span><br><span class="line">     norm0=initial_gradient*initial_gradient'; </span><br><span class="line">     syms step_zzh; </span><br><span class="line">     A=[x0]; </span><br><span class="line">     search_direction=-initial_gradient; </span><br><span class="line">     x=x0+step_zzh*search_direction; </span><br><span class="line">     f_step=subs(f,findsym(f),x); </span><br><span class="line">     best_step=golden_search(f_step,<span class="number">-15</span>,<span class="number">15</span>); </span><br><span class="line">     x_1=x0+best_step*search_direction; </span><br><span class="line">     A=[A;x_1]; </span><br><span class="line">     k=<span class="number">1</span>; </span><br><span class="line"><span class="keyword">while</span> norm0&gt;error </span><br><span class="line">     ox=x_1-x0; </span><br><span class="line">     og=gradient_my(f,x_1,b)-initial_gradient; </span><br><span class="line">     G1=G0+(ox'*ox)/(ox*og')-(G0*og'*og*G0)/(og*G0*og'); </span><br><span class="line">     <span class="keyword">if</span>  k+<span class="number">1</span>==b </span><br><span class="line">         new_direction=-gradient_my(f,x_1,b); </span><br><span class="line">     <span class="keyword">else</span> </span><br><span class="line">         new_direction=-(G1*(gradient_my(f,x_1,b))')'; </span><br><span class="line">     <span class="keyword">end</span> </span><br><span class="line">     x=x_1+step_zzh*new_direction; </span><br><span class="line">     f_step=subs(f,findsym(f),x); </span><br><span class="line">     best_step=golden_search(f_step,<span class="number">-15</span>,<span class="number">15</span>) </span><br><span class="line">     x_2=x_1+best_step*new_direction </span><br><span class="line">     A=[A;x_2]; </span><br><span class="line">     initial_gradient=gradient_my(f,x_1,b); </span><br><span class="line">     norm0=initial_gradient*initial_gradient'; </span><br><span class="line">     x0=x_1;x_1=x_2; </span><br><span class="line">     G0=G1; </span><br><span class="line">     k=k+<span class="number">1</span>; </span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;  我们在接触具体的机器学习算法前，其实很有必要对优化问题进行一些介绍。
随着学习的深入，笔者越来越发现最优化方法的重要性，学习和工作中遇到的大多问题都可以建模成一种最优化模型进行求解，比如我们现在学习的机器学习算法，大部分的机器学习算法的本质都是建立优化模型，通过最优化方法对目标函数（或损失函数）进行优化，从而训练出最好的模型。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Mathematic" scheme="http://yoursite.com/tags/Mathematic/"/>
    
      <category term="优化算法" scheme="http://yoursite.com/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
